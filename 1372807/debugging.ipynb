{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2010-12-31'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "/home/tony/Desktop/My_repos/sec_filings/sec/lib/python3.7/site-packages/ipykernel_launcher.py:326: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2010-12-31/Schedule_of_Investments_1.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2010-12-31/Schedule_of_Investments_2.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2010-12-31/Schedule_of_Investments_3.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2010-12-31/Schedule_of_Investments_4.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2010-12-31/Schedule_of_Investments_5.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2010-12-31/Schedule_of_Investments_6.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2010-12-31/Schedule_of_Investments_7.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2011-12-31'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2011-12-31/Schedule_of_Investments_1.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2011-12-31/Schedule_of_Investments_2.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2011-12-31/Schedule_of_Investments_3.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2011-12-31/Schedule_of_Investments_4.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2011-12-31/Schedule_of_Investments_5.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2011-12-31/Schedule_of_Investments_6.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2011-12-31/Schedule_of_Investments_7.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2011-12-31/Schedule_of_Investments_8.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2009-12-31'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2009-12-31/Schedule_of_Investments_1.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2009-12-31/Schedule_of_Investments_2.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2009-12-31/Schedule_of_Investments_3.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2009-12-31/Schedule_of_Investments_4.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2009-12-31/Schedule_of_Investments_5.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2009-12-31/Schedule_of_Investments_6.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "Boolean index has wrong length: 13 instead of 15",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20319/3332115531.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;31m# logger.info(cik)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_20319/3332115531.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;31m# display(merged_pair_idxs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m             \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerged_pair_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoi_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mexcept_rows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mex_rows\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerged_pair_idxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged_pair_idxs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0msoi_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m             \u001b[0mdfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             index_list = df.apply(\n",
      "\u001b[0;32m/tmp/ipykernel_20319/3332115531.py\u001b[0m in \u001b[0;36m_clean\u001b[0;34m(file_path, except_rows, merged_pair_idxs)\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mimportant_fields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrip_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_header_rows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#get_key_fields(df)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportant_fields\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerge_pair_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_duplicate_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerged_pair_idxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged_pair_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \u001b[0mduplicate_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'coerce'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mduplicated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0mclean_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mduplicate_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremove_row_duplicates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_20319/3332115531.py\u001b[0m in \u001b[0;36mmerge_duplicate_columns\u001b[0;34m(df, merged_pair_idxs)\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcol_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mmerged_pair_idxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0mduplicate_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m         \u001b[0mmerged_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mduplicate_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/My_repos/sec_filings/sec/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/My_repos/sec_filings/sec/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multi_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple_same_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/My_repos/sec_filings/sec/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple_same_dim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    804\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m             \u001b[0;31m# We should never have retval.ndim < self.ndim, as that should\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0;31m#  be handled by the _getitem_lowerdim call above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/My_repos/sec_filings/sec/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1142\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/My_repos/sec_filings/sec/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getbool_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0;31m# caller is responsible for ensuring non-None axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m         \u001b[0minds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/My_repos/sec_filings/sec/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36mcheck_bool_indexer\u001b[0;34m(index, key)\u001b[0m\n\u001b[1;32m   2399\u001b[0m         \u001b[0;31m# key may contain nan elements, check_array_indexer needs bool array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2400\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2401\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcheck_array_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/My_repos/sec_filings/sec/lib/python3.7/site-packages/pandas/core/indexers.py\u001b[0m in \u001b[0;36mcheck_array_indexer\u001b[0;34m(array, indexer)\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             raise IndexError(\n\u001b[0;32m--> 562\u001b[0;31m                 \u001b[0;34mf\"Boolean index has wrong length: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m                 \u001b[0;34mf\"{len(indexer)} instead of {len(array)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             )\n",
      "\u001b[0;31mIndexError\u001b[0m: Boolean index has wrong length: 13 instead of 15"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "sys.path.insert(0, '../') \n",
    "# from utils import init_logger\n",
    "\n",
    "def common_subheaders()->tuple:\n",
    "    return tuple(map(lambda header:header.replace(' ', r'\\s*'),\n",
    "        ('Advertising, Public Relations and Marketing ',\n",
    "        'Air Transportation',\n",
    "        'Amusement and Recreation',\n",
    "        'Apparel Manufacturing',\n",
    "        'Building Equipment Contractors',\n",
    "        'Business Support Services',\n",
    "        'Chemicals',\n",
    "        'Communications Equipment Manufacturing',\n",
    "        'Credit Related Activities',\n",
    "        'Computer Systems Design and Related Services',\n",
    "        'Credit (Nondepository)',\n",
    "        'Data Processing and Hosting Services',\n",
    "        'Educational Support Services',\n",
    "        'Electronic Component Manufacturing',\n",
    "        'Equipment Leasing',\n",
    "        'Facilities Support Services',\n",
    "        'Grocery Stores',\n",
    "        'Hospitals',\n",
    "        'Insurance',\n",
    "        'Lessors of Nonfinancial Licenses',\n",
    "        'Management, Scientific, and Technical Consulting Services',\n",
    "        'Motion Picture and Video Industries',\n",
    "        'Other Information Services',\n",
    "        'Other Manufacturing',\n",
    "        'Other Publishing',\n",
    "        'Other Real Estate Activities',\n",
    "        'Other Telecommunications',\n",
    "        'Plastics Manufacturing',\n",
    "        'Radio and Television Broadcasting',\n",
    "        'Real Estate Leasing',\n",
    "        'Restaurants',\n",
    "        'Retail',\n",
    "        'Satellite Telecommunications',\n",
    "        'Scientific Research and Development Services',\n",
    "        'Texttile Furnishings Mills',\n",
    "        'Traveler Arrangement',\n",
    "        'Software Publishing',\n",
    "        'Utility System Construction',\n",
    "        'Wholesalers',\n",
    "        'Wired Telecommunications Carriers',\n",
    "        'Wireless Telecommunications Carriers',\n",
    "        )\n",
    "    ))\n",
    "\n",
    "\n",
    "def standard_field_names()->tuple:\n",
    "    return (\n",
    "        'Portfolio Company',\n",
    "        # 'Portfolio Company /Principal Business',\n",
    "        'Investment /Interest Rate /Maturity',\n",
    "        'Percentage  Interest/  Shares',\n",
    "        'Principal',\n",
    "        'Cost',\n",
    "        'Value',\n",
    "        'Percentage Ownership',\n",
    "        'Percent of Class Held',\n",
    "        # 'Investment',\n",
    "        'CDO Fund Investments',\n",
    "        'Percent of Interests Held',\n",
    "        # 'Industry',\n",
    "        'Spread Above Index',\n",
    "        'Aquisition Date',\n",
    "        # 'Maturity',\n",
    "        # 'Principal/Shares',\n",
    "        # 'Investment Type',\n",
    "        'of Net Assets',\n",
    "        # 'business description',\n",
    "        # 'type of investment',\n",
    "        # 'investment date',\n",
    "        'reference rate and spread',\n",
    "        'pik rate',\n",
    "        # 'maturity date',\n",
    "        # 'cost',\n",
    "        'footnotes',\n",
    "        # 'industry',\n",
    "        # 'principal amount',\n",
    "        # 'fair value',\n",
    "    )\n",
    "\n",
    "    \n",
    "def company_control_headers()->tuple:\n",
    "    return tuple(map(lambda header:header.replace(' ', r'\\s*'),\n",
    "        (\n",
    "        'Debt Investments',\n",
    "        'Debt Investments (82.23%)',\n",
    "        'Debt Investments (A)',\n",
    "        'Debt Investments (continued)',\n",
    "        'Equity Securities',\n",
    "        'Equity Securities (continued)',\n",
    "        'Cash and Cash Equivalents',\n",
    "        )\n",
    "    ))\n",
    "\n",
    "def exceptions()->dict:\n",
    "    return {\n",
    "        '2006-12-31\\\\Schedule_of_Investments_1.csv':dict(),\n",
    "        '2006-12-31\\\\Schedule_of_Investments_3.csv':dict(), \n",
    "        '2008-03-31\\\\Schedule_of_Investments_7.csv':dict(),\n",
    "        '2008-03-31\\\\Schedule_of_Investments_8.csv':dict(),\n",
    "        '2008-12-31\\\\Schedule_of_Investments_6.csv':dict(),\n",
    "        '2008-12-31\\\\Schedule_of_Investments_11.csv':dict(),\n",
    "        '2008-12-31\\\\Schedule_of_Investments_13.csv':dict(),\n",
    "        '2008-12-31\\\\Schedule_of_Investments_14.csv': {\n",
    "                    'Portfolio_Company_/Principal_Business': np.array([ True, False, False, False, False, False, False, False, False,False, False, False, False, False,False]),\n",
    "                    '': np.array([ True, False,  True, True, False, True,  True, True, False,True,  True, True, False, True,False]),\n",
    "                    'Investment': np.array([ True, False, False, False, False, False]),\n",
    "                    'Percent_of_Interests_Held': np.array([ True,  False,  False, False, False, False]),\n",
    "                    'Cost': np.array([True,  False, False, False, False, False]),\n",
    "                    'Value': np.array([True,  False, False, False, False, False])\n",
    "                },\n",
    "        '2009-03-31\\\\Schedule_of_Investments_2.csv':dict(),\n",
    "        '2009-03-31\\\\Schedule_of_Investments_5.csv':dict(),\n",
    "        '2009-03-31\\\\Schedule_of_Investments_6.csv':dict(),\n",
    "        '2009-03-31\\\\Schedule_of_Investments_9.csv':dict(),\n",
    "        '2009-09-30\\\\Schedule_of_Investments_8.csv':dict(),\n",
    "        '2009-12-31\\\\Schedule_of_Investments_6.csv':dict(),\n",
    "        '2009-12-31\\\\Schedule_of_Investments_7.csv':dict(),\n",
    "        '2009-12-31\\\\Schedule_of_Investments_12.csv': {\n",
    "            'Portfolio Company': np.array([ True, False, False, False, False, False, False, False, False,False, False, False, False, False]),\n",
    "            'Investment /Interest Rate /Maturity': np.array([ False, True, False, False, False, False, False, False, False,False, False, False, False, False]),\n",
    "            'Percent_of_Interests_Held':np.array([False,False,True,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "            'Cost':np.array([False,False,False,False,True,True,False,False,False,False,False,False,False,False]),\n",
    "            'Value':np.array([False,False,False,False,False,False,True,True,False,False,False,False,False]),\n",
    "            '':np.array([True]*6 + [False]*6),\n",
    "        },\n",
    "        '2010-03-31\\\\Schedule_of_Investments_7.csv':dict(),\n",
    "        '2010-03-31\\\\Schedule_of_Investments_8.csv':dict(),\n",
    "        '2010-06-30\\\\Schedule_of_Investments_19.csv':dict(),\n",
    "        '2011-06-30\\\\Schedule_of_Investments_4.csv':dict(),\n",
    "        '2011-06-30\\\\Schedule_of_Investments_6.csv':dict(),\n",
    "        '2011-06-30\\\\Schedule_of_Investments_9.csv':dict(),\n",
    "        '2011-12-31\\\\Schedule_of_Investments_9.csv':dict(),\n",
    "        '2012-03-31\\\\Schedule_of_Investments_27.csv':dict(),\n",
    "        '2018-09-30\\\\Schedule_of_Investments_2.csv':dict(),\n",
    "        '2019-03-31\\\\Schedule_of_Investments_14.csv':dict(),\n",
    "        '2020-12-31\\\\Schedule_of_Investments_16.csv':{\n",
    "            'Investment':np.array([True,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "            'Cost':np.array([False,False,False,False,False,False,False,False,True,False,False,False,False,False,False]),\n",
    "            'Value':np.array([False,False,False,False,False,False,False,False,False,False,False,True,False,False,False]),\n",
    "            '':np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "        },\n",
    "        '2021-03-31\\\\Schedule_of_Investments_16.csv': {\n",
    "            'Investment':np.array([True,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "            'Cost':np.array([False,False,False,False,False,False,False,False,True,False,False,False,False,False,False]),\n",
    "            'Value':np.array([False,False,False,False,False,False,False,False,False,False,False,True,False,False,False]),\n",
    "            '':np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "        },\n",
    "        '2021-06-30\\\\Schedule_of_Investments_16.csv':{\n",
    "            'Investment':np.array([True,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "            'Cost':np.array([False,False,False,False,False,False,False,False,True,False,False,False,False,False,False]),\n",
    "            'Value':np.array([False,False,False,False,False,False,False,False,False,False,False,True,False,False,False]),\n",
    "            '':np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "        },\n",
    "        '2021-09-30\\\\Schedule_of_Investments_12.csv':dict(),\n",
    "        '2021-09-30\\\\Schedule_of_Investments_13.csv':dict(),\n",
    "        '2021-09-30\\\\Schedule_of_Investments_14.csv': {\n",
    "            'Investment':np.array([True,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "            'Cost':np.array([False,False,False,False,False,False,False,False,True,False,False,False,False,False,False]),\n",
    "            'Value':np.array([False,False,False,False,False,False,False,False,False,False,False,True,False,False,False]),\n",
    "            '':np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "        },\n",
    "        '2022-03-31\\\\Schedule_of_Investments_5.csv':dict(),\n",
    "        '2022-03-31\\\\Schedule_of_Investments_6.csv':dict(),\n",
    "        '2022-06-30\\\\Schedule_of_Investments_4.csv':dict(),\n",
    "        '2022-06-30\\\\Schedule_of_Investments_6.csv':dict(),\n",
    "        '2022-06-30\\\\Schedule_of_Investments_7.csv':dict(),\n",
    "        '2022-09-30\\\\Schedule_of_Investments_4.csv':dict(),\n",
    "        '2022-09-30\\\\Schedule_of_Investments_5.csv':dict(),\n",
    "        '2022-09-30\\\\Schedule_of_Investments_6.csv':dict(),\n",
    "        '2022-09-30\\\\Schedule_of_Investments_9.csv':dict(),\n",
    "        '2022-09-30\\\\Schedule_of_Investments_14.csv':dict(),\n",
    "        '2022-12-31\\\\Schedule_of_Investments_4.csv':dict(),\n",
    "        '2022-12-31\\\\Schedule_of_Investments_5.csv':dict(),\n",
    "        '2022-12-31\\\\Schedule_of_Investments_6.csv':dict(),\n",
    "        '2023-03-31\\\\Schedule_of_Investments_4.csv':dict(),\n",
    "        '2023-03-31\\\\Schedule_of_Investments_5.csv':dict(),\n",
    "        '2023-03-31\\\\Schedule_of_Investments_6.csv':dict(),\n",
    "        '2023-06-30\\\\Schedule_of_Investments_4.csv':dict(),\n",
    "        '2023-06-30\\\\Schedule_of_Investments_5.csv':dict(),\n",
    "        '2023-06-30\\\\Schedule_of_Investments_6.csv':dict(),\n",
    "        '2023-09-30\\\\Schedule_of_Investments_4.csv':dict(),\n",
    "        '2023-09-30\\\\Schedule_of_Investments_5.csv':dict(),\n",
    "        '2023-09-30\\\\Schedule_of_Investments_6.csv':dict(),\n",
    "        '2023-12-31\\\\Schedule_of_Investments_4.csv':dict(),\n",
    "        '2023-12-31\\\\Schedule_of_Investments_5.csv':dict(),\n",
    "        '2023-12-31\\\\Schedule_of_Investments_6.csv':dict(),\n",
    "        '2024-03-31\\\\Schedule_of_Investments_1.csv':dict(),\n",
    "        '2024-03-31\\\\Schedule_of_Investments_4.csv':dict(),\n",
    "        '2024-03-31\\\\Schedule_of_Investments_5.csv':dict(),\n",
    "        '2024-03-31\\\\Schedule_of_Investments_6.csv':dict(),\n",
    "    }\n",
    "\n",
    "def except_rows()->tuple:\n",
    "    return (\n",
    "        'asdf',\n",
    "    )\n",
    "\n",
    "# https://www.sec.gov/robots.txt\n",
    "def get_standard_name(col, choices, score_cutoff=60):\n",
    "    best_match, score = process.extractOne(col, choices)\n",
    "    if score > score_cutoff:\n",
    "        return best_match\n",
    "    return col\n",
    "\n",
    "def stopping_criterion(qtr:str)->str:\n",
    "    return '{}'.format(r'Total_*Investments')\n",
    "\n",
    " \n",
    "def concat(*dfs)->list:\n",
    "    final = []\n",
    "    for df in dfs:\n",
    "        final.extend(df.values.tolist())\n",
    "    return final\n",
    "\n",
    "    \n",
    "def get_key_fields(\n",
    "    df_cur:pd.DataFrame,\n",
    ")->tuple:\n",
    "    important_fields = standard_field_names() + common_subheaders()\n",
    "    for idx,row in enumerate(df_cur.iterrows()):\n",
    "        found = any(any(\n",
    "            key in str(field).lower() \n",
    "            for key in important_fields)\n",
    "                    for field in row[-1].dropna().tolist()\n",
    "            )\n",
    "        if found and len(set(row[-1].dropna().tolist())) >= 6:\n",
    "            cols = df_cur.iloc[:idx + 1].apply(lambda row: ' '.join(row.dropna()), axis=0).tolist()\n",
    "            fields = strip_string(cols,standardize=found) \n",
    "            return fields\n",
    "    return strip_string(df_cur.iloc[0].tolist())\n",
    "\n",
    "def strip_string(\n",
    "    columns_names:list,\n",
    "    standardize:bool=False\n",
    ")->tuple:\n",
    "    # columns = tuple(map(lambda col:re.sub(r'[^a-z]', '', str(col).lower()),columns_names))\n",
    "    if standardize:\n",
    "        standard_fields = standard_field_names()\n",
    "        return tuple(\n",
    "            re.sub(r'\\s+', '_',get_standard_name(str(col),standard_fields)) for col in columns_names\n",
    "        )\n",
    "    return tuple(re.sub(r'\\s+', '_',str(col)) for col in columns_names)\n",
    "\n",
    "\n",
    "# Function to extract date and convert to datetime object\n",
    "def extract_date(file_path):\n",
    "    # Extract date from file path (assuming date is always in 'YYYY-MM-DD' format)\n",
    "    date_str = re.search(r'\\d{4}-\\d{2}-\\d{2}', file_path).group()\n",
    "    return datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "\n",
    "\n",
    "def merge_duplicate_columns(\n",
    "    df:pd.DataFrame,\n",
    "    merged_pair_idxs:dict={}\n",
    ")->pd.DataFrame:\n",
    "    duplicate_cols = merged_pair_idxs.keys()\n",
    "    flag = not merged_pair_idxs.keys()\n",
    "    if flag: \n",
    "        duplicate_cols = df.columns.unique() \n",
    "    for col_name in duplicate_cols:\n",
    "        # display(col_name)\n",
    "        mask = merged_pair_idxs.get(col_name)\n",
    "        if flag:\n",
    "            mask = df.columns == col_name\n",
    "            merged_pair_idxs[col_name] = mask\n",
    "        duplicate_data = df.loc[:, mask]\n",
    "        merged_data = duplicate_data.apply(lambda row: ' '.join(set(row.dropna().astype(str))), axis=1)\n",
    "        df = df.loc[:, ~mask]\n",
    "        df[col_name] = merged_data\n",
    "        # display(df)\n",
    "    return df.reset_index(drop=True),merged_pair_idxs\n",
    "\n",
    "def extract_subheaders(\n",
    "    df:pd.DataFrame,\n",
    "    control:bool,\n",
    ")->pd.DataFrame:\n",
    "    col_name = 'company_control' if control else 'Type_of_Investment'\n",
    "    if col_name in df.columns:\n",
    "        return df\n",
    "    include = df.apply(\n",
    "        lambda row: re.search('|'.join(company_control_headers() if control else common_subheaders()), str(row[0]), re.IGNORECASE) is not None,\n",
    "        axis=1\n",
    "    )  \n",
    "    \n",
    "    exclude = ~df.apply(\n",
    "        lambda row: row.astype(str).str.contains('total|Inc|Ltd|LLC|Holdings|LP|Co|Corporation', case=False, na=False).any(),\n",
    "        axis=1\n",
    "    )\n",
    "    idx = df[include & exclude].index.tolist()\n",
    "    df[col_name] = None\n",
    "    if not idx:\n",
    "        return df\n",
    "\n",
    "    prev_header = subheader = None\n",
    "    df.loc[idx[-1]:,col_name] = df.iloc[idx[-1],1] if isinstance(df.iloc[idx[-1],0],float)  else df.iloc[idx[-1],0]\n",
    "    for j,i in enumerate(idx[:-1]):\n",
    "        prev_header = subheader\n",
    "        subheader = df.iloc[i,1] if isinstance(df.iloc[i,0],float)  else df.iloc[i,0]\n",
    "        df.loc[idx[j]:idx[j+1],col_name] = subheader if subheader != '' else prev_header\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_row_duplicates(row:pd.Series)->pd.Series: \n",
    "    out = []\n",
    "    for v in row:\n",
    "        if v in out and not str(v).replace('$','').isnumeric():\n",
    "            out.append(np.nan)\n",
    "        else:\n",
    "            out.append(v)\n",
    "    return pd.Series(out)\n",
    "\n",
    "  \n",
    "\n",
    "def _clean(\n",
    "    file_path:str,\n",
    "    except_rows:str,\n",
    "    merged_pair_idxs:dict={},\n",
    ")->pd.DataFrame:\n",
    "    df = pd.read_csv(file_path,index_col=0,na_values=[' ', ''])\n",
    "    df.replace(to_replace=r'[\\[\\](){},$%˄\\xa0\\u200b]', value='', regex=True,inplace=True)\n",
    "    df.replace(['PrincipalBusiness',' '],'_',regex=True,inplace=True)\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    df.dropna(axis=0,how='all',inplace=True)\n",
    "    df = df[~df.apply(lambda row:row.astype(str).str.contains(except_rows,case=False, na=False).any(),axis=1)]\n",
    "    if not df.apply(lambda col: col.astype(str).str.contains(r'total_*investments', case=False, regex=True)).any().any() and df.shape[0] < 3:\n",
    "        return pd.DataFrame(), merged_pair_idxs\n",
    "    \n",
    "    if not merged_pair_idxs:\n",
    "        important_fields = strip_string(get_header_rows(df),standardize=True)#get_key_fields(df)\n",
    "        df.columns = important_fields\n",
    "    df,merge_pair_idxs = merge_duplicate_columns(df,merged_pair_idxs=merged_pair_idxs)\n",
    "    duplicate_idx = df.apply(lambda row:row[pd.to_numeric(row,errors='coerce').isna()].duplicated().sum() > 1 ,axis=1)\n",
    "    clean_rows = df.loc[duplicate_idx].apply(remove_row_duplicates, axis=1).reset_index(drop=True)\n",
    "    j = 0\n",
    "    for i,flag in enumerate(duplicate_idx):\n",
    "        if not flag:\n",
    "            continue\n",
    "        df.iloc[i,:] = clean_rows.loc[j,:]\n",
    "        j += 1\n",
    "    df.replace([''],np.nan,regex=True,inplace=True) #':','$','%'\n",
    "    df.dropna(axis=1,how='all',inplace=True)\n",
    "    \n",
    "    columns = (~df.isna()).sum(axis=0) <= (4  if df.shape[0] > 12 else 2 if df.shape[0] == 4 else 0)\n",
    "    df = df.drop(columns=df.columns[columns])\n",
    "    return df.reset_index(drop=True),merge_pair_idxs\n",
    "\n",
    "\n",
    "\n",
    "def get_header_rows(\n",
    "    df_cur:pd.DataFrame,\n",
    ")->tuple:\n",
    "    for idx,row in df_cur.iterrows():\n",
    "        found = any(str(v).replace(\"$\",'').replace(\"%\",'').isnumeric() for v in row)\n",
    "        if found:\n",
    "            out = df_cur.iloc[:idx,:].apply(lambda row: ' '.join(row[row.notna()].values), axis=0)\n",
    "            return out\n",
    "    \n",
    "    return strip_string(df_cur.iloc[0].tolist())\n",
    "\n",
    "\n",
    "def main()->None:\n",
    "    qtrs = os.listdir('.')\n",
    "    ex = exceptions()\n",
    "    ex_rows = '|'.join(except_rows())\n",
    "    for qtr in qtrs:\n",
    "        if '.csv' in qtr or not os.path.exists(os.path.join(qtr,f'Schedule_of_Investments_0.csv')):\n",
    "            continue\n",
    "        # qtr = '2008-12-31'\n",
    "        # logger.info(qtr)\n",
    "        display(qtr)\n",
    "        index_list_sum = i = 0\n",
    "        soi_files = sorted([\n",
    "            os.path.join(qtr,file) \n",
    "            for file in os.listdir(qtr)\n",
    "            if file.endswith('.csv')\n",
    "        ],key=lambda f: int(f.split('_')[-1].split('.')[0]))\n",
    "        # soi_files = [f for f in soi_files] # if f not in ex]\n",
    "        if len(soi_files) == 0:\n",
    "            continue\n",
    "        df,merged_pair_idxs = _clean(soi_files[i],except_rows=ex_rows,merged_pair_idxs={})\n",
    "\n",
    "        index_list = df.apply(\n",
    "            lambda row:row.astype(str).str.contains(stopping_criterion(qtr), case=False, na=False).any(),\n",
    "            axis=1\n",
    "        )\n",
    "        index_list_sum = index_list.sum()\n",
    "        dfs = [df]     \n",
    "        i += 1\n",
    "\n",
    "        while index_list_sum == 0:\n",
    "            # logger.info(soi_files[i])\n",
    "            merged_pair_idxs = ex.get(soi_files[i],merged_pair_idxs)\n",
    "            display(soi_files[i])\n",
    "\n",
    "            # display(merged_pair_idxs)\n",
    "            df,merged_pair_idxs = _clean(soi_files[i],except_rows=ex_rows,merged_pair_idxs=merged_pair_idxs if soi_files[i] not in ex else {})\n",
    "            dfs.append(df)\n",
    "            index_list = df.apply(\n",
    "                lambda row:row.astype(str).str.contains(stopping_criterion(qtr), case=False, na=False).any(),\n",
    "                axis=1\n",
    "            )\n",
    "            index_list_sum = index_list.sum()\n",
    "            i += 1\n",
    "        date_final = dfs[0]\n",
    "        if len(dfs) > 1:\n",
    "            date_final = pd.concat(dfs,axis=0,ignore_index=True)#pd.DataFrame(concat(*dfs))\n",
    "        # date_final = extract_subheaders(date_final,control=True)\n",
    "        # date_final = extract_subheaders(date_final,control=False)\n",
    "\n",
    "        date_final['qtr'] = qtr.split('\\\\')[-1]\n",
    "        if not os.path.exists(os.path.join(qtr,'output')):\n",
    "            os.makedirs(os.path.join(qtr,'output'))\n",
    "        columns_to_drop = date_final.notna().sum() <= 2\n",
    "        date_final.drop(columns=columns_to_drop[columns_to_drop].index)\n",
    "        date_final.to_csv(os.path.join(qtr,'output',f'{qtr}.csv'),index=False)\n",
    "        # break\n",
    "    \n",
    "    # Use glob to find files\n",
    "    files = sorted(glob.glob(f'*/output/*.csv'), key=extract_date)\n",
    "    single_truth = pd.concat([\n",
    "        pd.read_csv(df) for df in files\n",
    "    ],axis=0,ignore_index=True)\n",
    "    single_truth.drop(columns=single_truth.columns[['Unnamed' in col for col in single_truth.columns]],inplace=True)\n",
    "    single_truth.to_csv(f'{cik}_soi_table.csv',index=False)\n",
    "    \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "cik = 1372807\n",
    "# logger = init_logger(cik)\n",
    "# logger.info(cik)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>5</th>\n",
       "      <th>Investment</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Investment__Interest_Rate¹_/_Maturity</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Portfolio_Company_/_Principal_Business</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Junior_Secured_Loan—Second_Lien__Term_Loan__13...</td>\n",
       "      <td>2013977</td>\n",
       "      <td>Rhodes_Companies_LLC_The__6_9__Buildings_and_R...</td>\n",
       "      <td>2019403</td>\n",
       "      <td>12720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Junior_Secured_Loan—Loan_Second_Lien__5.8_Due_...</td>\n",
       "      <td>3000000</td>\n",
       "      <td>San_Juan_Cable_LLC__6__Broadcasting_and_Entert...</td>\n",
       "      <td>2986206</td>\n",
       "      <td>2862000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Senior_Secured_Loan—Term_Loan__3.7_Due_6/13</td>\n",
       "      <td>4304174</td>\n",
       "      <td>Schneller_LLC__6__Aerospace_and_Defense</td>\n",
       "      <td>4278271</td>\n",
       "      <td>4231003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Senior_Secured_Loan—Term_Loan__3.2_Due_6/12</td>\n",
       "      <td>838694</td>\n",
       "      <td>Seismic_Micro-Technology_Inc._SMT__6__Electronics</td>\n",
       "      <td>837545</td>\n",
       "      <td>829469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Senior_Secured_Loan—Term_Loan__3.2_Due_6/12</td>\n",
       "      <td>1258041</td>\n",
       "      <td>Seismic_Micro-Technology_Inc._SMT__6__Electronics</td>\n",
       "      <td>1256317</td>\n",
       "      <td>1244203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Junior_Secured_Loan—Loan_Second_Lien__7.2_Due_...</td>\n",
       "      <td>7500000</td>\n",
       "      <td>Specialized_Technology_Resources_Inc.__6__Dive...</td>\n",
       "      <td>7500000</td>\n",
       "      <td>7500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Senior_Secured_Loan—Term__Loan_First_Lien__2.7...</td>\n",
       "      <td>3563166</td>\n",
       "      <td>Specialized_Technology_Resources_Inc.__6__Dive...</td>\n",
       "      <td>3563166</td>\n",
       "      <td>3563166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Senior_Secured_Loan—Delayed_Draw_Term_Loan__8....</td>\n",
       "      <td>738349</td>\n",
       "      <td>Standard_Steel_LLC__6__Cargo_Transport</td>\n",
       "      <td>741143</td>\n",
       "      <td>738349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Senior_Secured_Loan—Initial_Term_Loan__9.0_Due...</td>\n",
       "      <td>3663267</td>\n",
       "      <td>Standard_Steel_LLC__6__Cargo_Transport</td>\n",
       "      <td>3677125</td>\n",
       "      <td>3663267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Junior_Secured_Loan—Loan_Second_Lien__13.8_Due...</td>\n",
       "      <td>1750000</td>\n",
       "      <td>Standard_Steel_LLC__6__Cargo_Transport</td>\n",
       "      <td>1756512</td>\n",
       "      <td>1750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Junior_Secured_Loan—Loan_Second_Lien__4.5_Due_...</td>\n",
       "      <td>2000000</td>\n",
       "      <td>TPF_Generation_Holdings_LLC__6__Utilities</td>\n",
       "      <td>2023571</td>\n",
       "      <td>1894000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Senior_Secured_Loan—Term_Loan__First_Lien__3.2...</td>\n",
       "      <td>3590407</td>\n",
       "      <td>TUI_University_LLC__6__Healthcare_Education_an...</td>\n",
       "      <td>3465781</td>\n",
       "      <td>3590407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Senior_Secured_Loan—Term_Loan__4.3_Due_4/13</td>\n",
       "      <td>4291879</td>\n",
       "      <td>Twin-Star_International_Inc.__6__Home_and_Offi...</td>\n",
       "      <td>4291879</td>\n",
       "      <td>4218917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Senior_Secured_Loan—Term_Loan__4.3_Due_4/13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Twin-Star_International_Inc.__6__Home_and_Offi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Junior_Secured_Loan—Term_Loan_B__12.5_Due_12/12</td>\n",
       "      <td>526500</td>\n",
       "      <td>Walker_Group_Holdings_LLC__Cargo_Transport</td>\n",
       "      <td>526500</td>\n",
       "      <td>526500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    2        5  \\\n",
       "0               Investment__Interest_Rate¹_/_Maturity      NaN   \n",
       "1   Junior_Secured_Loan—Second_Lien__Term_Loan__13...  2013977   \n",
       "2   Junior_Secured_Loan—Loan_Second_Lien__5.8_Due_...  3000000   \n",
       "3         Senior_Secured_Loan—Term_Loan__3.7_Due_6/13  4304174   \n",
       "4         Senior_Secured_Loan—Term_Loan__3.2_Due_6/12   838694   \n",
       "5         Senior_Secured_Loan—Term_Loan__3.2_Due_6/12  1258041   \n",
       "6   Junior_Secured_Loan—Loan_Second_Lien__7.2_Due_...  7500000   \n",
       "7   Senior_Secured_Loan—Term__Loan_First_Lien__2.7...  3563166   \n",
       "8   Senior_Secured_Loan—Delayed_Draw_Term_Loan__8....   738349   \n",
       "9   Senior_Secured_Loan—Initial_Term_Loan__9.0_Due...  3663267   \n",
       "10  Junior_Secured_Loan—Loan_Second_Lien__13.8_Due...  1750000   \n",
       "11  Junior_Secured_Loan—Loan_Second_Lien__4.5_Due_...  2000000   \n",
       "12  Senior_Secured_Loan—Term_Loan__First_Lien__3.2...  3590407   \n",
       "13        Senior_Secured_Loan—Term_Loan__4.3_Due_4/13  4291879   \n",
       "14        Senior_Secured_Loan—Term_Loan__4.3_Due_4/13      NaN   \n",
       "15    Junior_Secured_Loan—Term_Loan_B__12.5_Due_12/12   526500   \n",
       "\n",
       "                                           Investment     Cost    Value  \n",
       "0              Portfolio_Company_/_Principal_Business      NaN      NaN  \n",
       "1   Rhodes_Companies_LLC_The__6_9__Buildings_and_R...  2019403    12720  \n",
       "2   San_Juan_Cable_LLC__6__Broadcasting_and_Entert...  2986206  2862000  \n",
       "3             Schneller_LLC__6__Aerospace_and_Defense  4278271  4231003  \n",
       "4   Seismic_Micro-Technology_Inc._SMT__6__Electronics   837545   829469  \n",
       "5   Seismic_Micro-Technology_Inc._SMT__6__Electronics  1256317  1244203  \n",
       "6   Specialized_Technology_Resources_Inc.__6__Dive...  7500000  7500000  \n",
       "7   Specialized_Technology_Resources_Inc.__6__Dive...  3563166  3563166  \n",
       "8              Standard_Steel_LLC__6__Cargo_Transport   741143   738349  \n",
       "9              Standard_Steel_LLC__6__Cargo_Transport  3677125  3663267  \n",
       "10             Standard_Steel_LLC__6__Cargo_Transport  1756512  1750000  \n",
       "11          TPF_Generation_Holdings_LLC__6__Utilities  2023571  1894000  \n",
       "12  TUI_University_LLC__6__Healthcare_Education_an...  3465781  3590407  \n",
       "13  Twin-Star_International_Inc.__6__Home_and_Offi...  4291879  4218917  \n",
       "14  Twin-Star_International_Inc.__6__Home_and_Offi...      NaN      NaN  \n",
       "15         Walker_Group_Holdings_LLC__Cargo_Transport   526500   526500  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Investment': array([ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False]),\n",
       " 'Cost': array([False, False, False, False, False, False, False, False,  True,\n",
       "        False, False, False, False, False, False]),\n",
       " 'Value': array([False, False, False, False, False, False, False, False, False,\n",
       "        False, False,  True, False, False, False]),\n",
       " '': array([False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = r'2009-12-31/Schedule_of_Investments_6.csv'\n",
    "\n",
    "\n",
    "\n",
    "def standard_field_names()->tuple:\n",
    "    return (\n",
    "        'Portfolio Company',\n",
    "        # 'Portfolio Company /Principal Business',\n",
    "        'Percentage  Interest/  Shares',\n",
    "        'Investment /Interest Rate /Maturity',        \n",
    "        'Principal',\n",
    "        'Cost',\n",
    "        'Value',\n",
    "        'Percentage Ownership',\n",
    "        'Initial  Acquisition  Date',\n",
    "        'Percent of Class Held',\n",
    "        'Investment',\n",
    "        'CDO Fund Investments',\n",
    "        'Percent of Interests Held',\n",
    "        'Industry',\n",
    "        'Spread Above Index',\n",
    "        'Aquisition Date',\n",
    "        'Interest Rate',\n",
    "        'Maturity',\n",
    "        'Principal/Shares',\n",
    "        'Investment Type',\n",
    "        'of Net Assets',\n",
    "        'business description',\n",
    "        'type of investment',\n",
    "        'investment date',\n",
    "        'reference rate and spread',\n",
    "        'pik rate',\n",
    "        'maturity date',\n",
    "        'cost',\n",
    "        'footnotes',\n",
    "        'industry',\n",
    "        'principal amount', # TODO change stand names for more dynamic fuzzywuzzy matching\n",
    "        'fair value',\n",
    "    )\n",
    "\n",
    "def merge_duplicate_columns(\n",
    "    df:pd.DataFrame,\n",
    "    merged_pair_idxs:dict={}\n",
    ")->pd.DataFrame:\n",
    "    duplicate_cols = merged_pair_idxs.keys()\n",
    "    flag = not merged_pair_idxs.keys()\n",
    "    if flag: \n",
    "        duplicate_cols = df.columns.unique() \n",
    "    for col_name in duplicate_cols:\n",
    "        # display(col_name)\n",
    "        mask = merged_pair_idxs.get(col_name)\n",
    "        if flag:\n",
    "            mask = df.columns == col_name\n",
    "            merged_pair_idxs[col_name] = mask\n",
    "        duplicate_data = df.loc[:, mask]\n",
    "        merged_data = duplicate_data.apply(lambda row: ' '.join(set(row.dropna().astype(str))), axis=1)\n",
    "        df = df.loc[:, ~mask]\n",
    "        df[col_name] = merged_data\n",
    "        # display(df)\n",
    "    return df.reset_index(drop=True),merged_pair_idxs\n",
    "\n",
    "def _clean(\n",
    "    file_path:str,\n",
    "    except_rows:str,\n",
    "    merged_pair_idxs:dict={},\n",
    ")->pd.DataFrame:\n",
    "    df = pd.read_csv(file_path,index_col=0,na_values=[' ', ''])\n",
    "    df.replace(to_replace=r'[\\[\\](){},$%˄\\xa0\\u200b]', value='', regex=True,inplace=True)\n",
    "    df.replace(['PrincipalBusiness',' '],'_',regex=True,inplace=True)\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    df.dropna(axis=0,how='all',inplace=True)\n",
    "    df = df[~df.apply(lambda row:row.astype(str).str.contains(except_rows,case=False, na=False).any(),axis=1)]\n",
    "    # display(df)\n",
    "    if not df.apply(lambda col: col.astype(str).str.contains(r'total_*investments', case=False, regex=True)).any().any() and df.shape[0] < 3:\n",
    "        return pd.DataFrame(), merged_pair_idxs\n",
    "    \n",
    "    if not merged_pair_idxs:\n",
    "        important_fields = strip_string(get_header_rows(df),standardize=True)#get_key_fields(df)\n",
    "        df.columns = important_fields\n",
    "    df,merge_pair_idxs = merge_duplicate_columns(df,merged_pair_idxs=merged_pair_idxs)\n",
    "    duplicate_idx = df.apply(lambda row:row[pd.to_numeric(row,errors='coerce').isna()].duplicated().sum() > 1 ,axis=1)\n",
    "    clean_rows = df.loc[duplicate_idx].apply(remove_row_duplicates, axis=1).reset_index(drop=True)\n",
    "    j = 0\n",
    "    for i,flag in enumerate(duplicate_idx):\n",
    "        if not flag:\n",
    "            continue\n",
    "        df.iloc[i,:] = clean_rows.loc[j,:]\n",
    "        j += 1\n",
    "    df.replace([''],np.nan,regex=True,inplace=True) #':','$','%'\n",
    "    df.dropna(axis=1,how='all',inplace=True)\n",
    "    \n",
    "    columns = (~df.isna()).sum(axis=0) <= (4  if df.shape[0] > 12 else 2 if df.shape[0] == 4 else 0)\n",
    "    df = df.drop(columns=df.columns[columns])\n",
    "    return df.reset_index(drop=True),merge_pair_idxs\n",
    "\n",
    "\n",
    "merged_pair_idxs = {\n",
    "    'Investment':np.array([True,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "    'Cost':np.array([False,False,False,False,False,False,False,False,True,False,False,False,False,False,False]),\n",
    "    'Value':np.array([False,False,False,False,False,False,False,False,False,False,False,True,False,False,False]),\n",
    "    '':np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "}\n",
    "# merged_pair_idxs = {}\n",
    "ex = exceptions()\n",
    "ex_rows = '|'.join(except_rows())\n",
    "df,merged_pair_idxs = _clean(file_path,except_rows=ex_rows,merged_pair_idxs=merged_pair_idxs)\n",
    "display(df)\n",
    "display(merged_pair_idxs)\n",
    "index_list = df.apply(\n",
    "    lambda row:row.astype(str).str.contains(stopping_criterion(None), case=False, na=False).any(),\n",
    "    axis=1\n",
    ")\n",
    "index_list_sum = index_list.sum()\n",
    "index_list_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Total Investments  4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$</td>\n",
       "      <td>536008160.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$</td>\n",
       "      <td>486628831.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     0   1   2   3   4   5   6   7    8  \\\n",
       "0           0                   NaN NaN NaN NaN NaN NaN NaN NaN  NaN   \n",
       "1           1  Total Investments  4 NaN NaN NaN NaN NaN NaN NaN    $   \n",
       "\n",
       "             9  10  11   12           13  14  \n",
       "0          NaN NaN NaN  NaN          NaN NaN  \n",
       "1  536008160.0 NaN NaN    $  486628831.0 NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(file_path)\n",
    "# df.apply(lambda col: col.astype(str).str.contains(r'total_*investments', case=False, regex=True)).any().any()\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sec_windows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
