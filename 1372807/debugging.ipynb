{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-12-31'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_1.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_2.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_3.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_4.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_5.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_6.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_7.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_8.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_9.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_10.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_11.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_12.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_13.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_14.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_15.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_16.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_17.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_18.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_19.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_20.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_21.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_22.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_23.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_24.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_25.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_26.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_27.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_28.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_29.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_30.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_31.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_32.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_33.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_34.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_35.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_36.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_37.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_38.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_39.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_40.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_41.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_42.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_43.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_44.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_45.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'2020-12-31\\\\Schedule_of_Investments_46.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "sys.path.insert(0, '../') \n",
    "# from utils import init_logger\n",
    "\n",
    "def common_subheaders()->tuple:\n",
    "    return tuple(map(lambda header:header.replace(' ', r'\\s*'),\n",
    "        ('Advertising, Public Relations and Marketing ',\n",
    "        'Air Transportation',\n",
    "        'Amusement and Recreation',\n",
    "        'Apparel Manufacturing',\n",
    "        'Building Equipment Contractors',\n",
    "        'Business Support Services',\n",
    "        'Chemicals',\n",
    "        'Communications Equipment Manufacturing',\n",
    "        'Credit Related Activities',\n",
    "        'Computer Systems Design and Related Services',\n",
    "        'Credit (Nondepository)',\n",
    "        'Data Processing and Hosting Services',\n",
    "        'Educational Support Services',\n",
    "        'Electronic Component Manufacturing',\n",
    "        'Equipment Leasing',\n",
    "        'Facilities Support Services',\n",
    "        'Grocery Stores',\n",
    "        'Hospitals',\n",
    "        'Insurance',\n",
    "        'Lessors of Nonfinancial Licenses',\n",
    "        'Management, Scientific, and Technical Consulting Services',\n",
    "        'Motion Picture and Video Industries',\n",
    "        'Other Information Services',\n",
    "        'Other Manufacturing',\n",
    "        'Other Publishing',\n",
    "        'Other Real Estate Activities',\n",
    "        'Other Telecommunications',\n",
    "        'Plastics Manufacturing',\n",
    "        'Radio and Television Broadcasting',\n",
    "        'Real Estate Leasing',\n",
    "        'Restaurants',\n",
    "        'Retail',\n",
    "        'Satellite Telecommunications',\n",
    "        'Scientific Research and Development Services',\n",
    "        'Texttile Furnishings Mills',\n",
    "        'Traveler Arrangement',\n",
    "        'Software Publishing',\n",
    "        'Utility System Construction',\n",
    "        'Wholesalers',\n",
    "        'Wired Telecommunications Carriers',\n",
    "        'Wireless Telecommunications Carriers',\n",
    "        )\n",
    "    ))\n",
    "\n",
    "\n",
    "def standard_field_names()->tuple:\n",
    "    return (\n",
    "        'Portfolio Company',\n",
    "        # 'Portfolio Company /Principal Business',\n",
    "        'Investment /Interest Rate /Maturity',\n",
    "        'Percentage  Interest/  Shares',\n",
    "        'Principal',\n",
    "        'Cost',\n",
    "        'Value',\n",
    "        'Percent of Class Held',\n",
    "        'Investment',\n",
    "        'CDO Fund Investments',\n",
    "        'Percent of Interests Held',\n",
    "        'Industry',\n",
    "        'Spread Above Index',\n",
    "        'Aquisition Date',\n",
    "        'Interest Rate',\n",
    "        'Maturity',\n",
    "        'Principal/Shares',\n",
    "        'Investment Type',\n",
    "        'of Net Assets',\n",
    "        'business description',\n",
    "        'type of investment',\n",
    "        'investment date',\n",
    "        'reference rate and spread',\n",
    "        'pik rate',\n",
    "        'maturity date',\n",
    "        'cost',\n",
    "        'footnotes',\n",
    "        'industry',\n",
    "        'principal amount', # TODO change stand names for more dynamic fuzzywuzzy matching\n",
    "        'fair value',\n",
    "    )\n",
    "\n",
    "    \n",
    "def company_control_headers()->tuple:\n",
    "    return tuple(map(lambda header:header.replace(' ', r'\\s*'),\n",
    "        (\n",
    "        'Debt Investments',\n",
    "        'Debt Investments (82.23%)',\n",
    "        'Debt Investments (A)',\n",
    "        'Debt Investments (continued)',\n",
    "        'Equity Securities',\n",
    "        'Equity Securities (continued)',\n",
    "        'Cash and Cash Equivalents',\n",
    "        )\n",
    "    ))\n",
    "\n",
    "def exceptions()->dict:\n",
    "    return {\n",
    "        '2006-12-31\\\\Schedule_of_Investments_1.csv':dict(),\n",
    "        '2006-12-31\\\\Schedule_of_Investments_3.csv':dict(), \n",
    "        '2008-03-31\\\\Schedule_of_Investments_7.csv':dict(),\n",
    "        '2008-03-31\\\\Schedule_of_Investments_8.csv':dict(),\n",
    "        '2008-12-31\\\\Schedule_of_Investments_6.csv':dict(),\n",
    "        '2008-12-31\\\\Schedule_of_Investments_11.csv':dict(),\n",
    "        '2008-12-31\\\\Schedule_of_Investments_13.csv':dict(),\n",
    "        '2008-12-31\\\\Schedule_of_Investments_14.csv': {\n",
    "                    'Portfolio_Company_/Principal_Business': np.array([ True, False, False, False, False, False, False, False, False,False, False, False, False, False,False]),\n",
    "                    '': np.array([ True, False,  True, True, False, True,  True, True, False,True,  True, True, False, True,False]),\n",
    "                    'Investment': np.array([ True, False, False, False, False, False]),\n",
    "                    'Percent_of_Interests_Held': np.array([ True,  False,  False, False, False, False]),\n",
    "                    'Cost': np.array([True,  False, False, False, False, False]),\n",
    "                    'Value': np.array([True,  False, False, False, False, False])\n",
    "                },\n",
    "        '2009-03-31\\\\Schedule_of_Investments_2.csv':dict(),\n",
    "        '2009-03-31\\\\Schedule_of_Investments_5.csv':dict(),\n",
    "        '2009-03-31\\\\Schedule_of_Investments_6.csv':dict(),\n",
    "        '2009-03-31\\\\Schedule_of_Investments_9.csv':dict(),\n",
    "        '2009-09-30\\\\Schedule_of_Investments_8.csv':dict(),\n",
    "        '2009-12-31\\\\Schedule_of_Investments_6.csv':dict(),\n",
    "        '2009-12-31\\\\Schedule_of_Investments_7.csv':dict(),\n",
    "        '2009-12-31\\\\Schedule_of_Investments_12.csv': {\n",
    "            'Portfolio Company': np.array([ True, False, False, False, False, False, False, False, False,False, False, False, False, False]),\n",
    "            'Investment /Interest Rate /Maturity': np.array([ False, True, False, False, False, False, False, False, False,False, False, False, False, False]),\n",
    "            'Percent_of_Interests_Held':np.array([False,False,True,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "            'Cost':np.array([False,False,False,False,True,True,False,False,False,False,False,False,False,False]),\n",
    "            'Value':np.array([False,False,False,False,False,False,True,True,False,False,False,False,False]),\n",
    "            '':np.array([True]*6 + [False]*6),\n",
    "        },\n",
    "        '2010-03-31\\\\Schedule_of_Investments_7.csv':dict(),\n",
    "        '2010-03-31\\\\Schedule_of_Investments_8.csv':dict(),\n",
    "        '2010-06-30\\\\Schedule_of_Investments_19.csv':dict(),\n",
    "        '2011-06-30\\\\Schedule_of_Investments_4.csv':dict(),\n",
    "        '2011-06-30\\\\Schedule_of_Investments_6.csv':dict(),\n",
    "        '2011-06-30\\\\Schedule_of_Investments_9.csv':dict(),\n",
    "        '2011-12-31\\\\Schedule_of_Investments_9.csv':dict(),\n",
    "        '2012-03-31\\\\Schedule_of_Investments_27.csv':dict()\n",
    "\n",
    "    }\n",
    "\n",
    "def except_rows()->tuple:\n",
    "    return (\n",
    "        'asdf',\n",
    "    )\n",
    "\n",
    "# https://www.sec.gov/robots.txt\n",
    "def get_standard_name(col, choices, score_cutoff=60):\n",
    "    best_match, score = process.extractOne(col, choices)\n",
    "    if score > score_cutoff:\n",
    "        return best_match\n",
    "    return col\n",
    "\n",
    "def stopping_criterion(qtr:str)->str:\n",
    "    return '{}'.format(r'Total\\s*Investments')\n",
    "\n",
    " \n",
    "def concat(*dfs)->list:\n",
    "    final = []\n",
    "    for df in dfs:\n",
    "        final.extend(df.values.tolist())\n",
    "    return final\n",
    "\n",
    "    \n",
    "def get_key_fields(\n",
    "    df_cur:pd.DataFrame,\n",
    ")->tuple:\n",
    "    important_fields = standard_field_names() + common_subheaders()\n",
    "    for idx,row in enumerate(df_cur.iterrows()):\n",
    "        found = any(any(\n",
    "            key in str(field).lower() \n",
    "            for key in important_fields)\n",
    "                    for field in row[-1].dropna().tolist()\n",
    "            )\n",
    "        if found and len(set(row[-1].dropna().tolist())) >= 6:\n",
    "            cols = df_cur.iloc[:idx + 1].apply(lambda row: ' '.join(row.dropna()), axis=0).tolist()\n",
    "            fields = strip_string(cols,standardize=found) \n",
    "            return fields\n",
    "    return strip_string(df_cur.iloc[0].tolist())\n",
    "\n",
    "def strip_string(\n",
    "    columns_names:list,\n",
    "    standardize:bool=False\n",
    ")->tuple:\n",
    "    # columns = tuple(map(lambda col:re.sub(r'[^a-z]', '', str(col).lower()),columns_names))\n",
    "    if standardize:\n",
    "        standard_fields = standard_field_names()\n",
    "        return tuple(\n",
    "            re.sub(r'\\s+', '_',get_standard_name(str(col),standard_fields)) for col in columns_names\n",
    "        )\n",
    "    return tuple(re.sub(r'\\s+', '_',str(col)) for col in columns_names)\n",
    "\n",
    "\n",
    "# Function to extract date and convert to datetime object\n",
    "def extract_date(file_path):\n",
    "    # Extract date from file path (assuming date is always in 'YYYY-MM-DD' format)\n",
    "    date_str = re.search(r'\\d{4}-\\d{2}-\\d{2}', file_path).group()\n",
    "    return datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "\n",
    "\n",
    "def merge_duplicate_columns(\n",
    "    df:pd.DataFrame,\n",
    "    merged_pair_idxs:dict={}\n",
    ")->pd.DataFrame:\n",
    "    duplicate_cols = merged_pair_idxs.keys()\n",
    "    flag = not merged_pair_idxs.keys()\n",
    "    if flag: \n",
    "        duplicate_cols = df.columns.unique() \n",
    "    for col_name in duplicate_cols:\n",
    "        # display(col_name)\n",
    "        mask = merged_pair_idxs.get(col_name)\n",
    "        if flag:\n",
    "            mask = df.columns == col_name\n",
    "            merged_pair_idxs[col_name] = mask\n",
    "        duplicate_data = df.loc[:, mask]\n",
    "        merged_data = duplicate_data.apply(lambda row: ' '.join(set(row.dropna().astype(str))), axis=1)\n",
    "        df = df.loc[:, ~mask]\n",
    "        df[col_name] = merged_data\n",
    "        # display(df)\n",
    "    return df.reset_index(drop=True),merged_pair_idxs\n",
    "\n",
    "def extract_subheaders(\n",
    "    df:pd.DataFrame,\n",
    "    control:bool,\n",
    ")->pd.DataFrame:\n",
    "    col_name = 'company_control' if control else 'Type_of_Investment'\n",
    "    if col_name in df.columns:\n",
    "        return df\n",
    "    include = df.apply(\n",
    "        lambda row: re.search('|'.join(company_control_headers() if control else common_subheaders()), str(row[0]), re.IGNORECASE) is not None,\n",
    "        axis=1\n",
    "    )  \n",
    "    \n",
    "    exclude = ~df.apply(\n",
    "        lambda row: row.astype(str).str.contains('total|Inc|Ltd|LLC|Holdings|LP|Co|Corporation', case=False, na=False).any(),\n",
    "        axis=1\n",
    "    )\n",
    "    idx = df[include & exclude].index.tolist()\n",
    "    df[col_name] = None\n",
    "    if not idx:\n",
    "        return df\n",
    "\n",
    "    prev_header = subheader = None\n",
    "    df.loc[idx[-1]:,col_name] = df.iloc[idx[-1],1] if isinstance(df.iloc[idx[-1],0],float)  else df.iloc[idx[-1],0]\n",
    "    for j,i in enumerate(idx[:-1]):\n",
    "        prev_header = subheader\n",
    "        subheader = df.iloc[i,1] if isinstance(df.iloc[i,0],float)  else df.iloc[i,0]\n",
    "        df.loc[idx[j]:idx[j+1],col_name] = subheader if subheader != '' else prev_header\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_row_duplicates(row:pd.Series)->pd.Series: \n",
    "    out = []\n",
    "    for v in row:\n",
    "        if v in out and not str(v).replace('$','').isnumeric():\n",
    "            out.append(np.nan)\n",
    "        else:\n",
    "            out.append(v)\n",
    "    return pd.Series(out)\n",
    "\n",
    "  \n",
    "\n",
    "def _clean(\n",
    "    file_path:str,\n",
    "    except_rows:str,\n",
    "    merged_pair_idxs:dict={},\n",
    ")->pd.DataFrame:\n",
    "    df = pd.read_csv(file_path,index_col=0,na_values=[' ', ''])\n",
    "    # df.replace(['\\xa0','\\u200b',r'^\\s$',r'^\\s%'],np.nan,regex=True,inplace=True) #':','$','%' r'^\\s*$',r'^\\s*%'\n",
    "    df.replace(['\\xa0','\\u200b',r'^\\s*$',r'^\\s*%',' ','˄'],'',regex=True,inplace=True)\n",
    "    df.replace(['PrincipalBusiness'],'',regex=True,inplace=True)\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    df.dropna(axis=0,how='all',inplace=True)\n",
    "    df = df[~df.apply(lambda row:row.astype(str).str.contains(except_rows,case=False, na=False).any(),axis=1)]\n",
    "    if df.shape[0] < 3:\n",
    "        return pd.DataFrame(), merged_pair_idxs\n",
    "    if not merged_pair_idxs:\n",
    "        important_fields = strip_string(get_header_rows(df),standardize=True)#get_key_fields(df)\n",
    "        df.columns = important_fields\n",
    "    df,merge_pair_idxs = merge_duplicate_columns(df,merged_pair_idxs=merged_pair_idxs)\n",
    "    duplicate_idx = df.apply(lambda row:row[pd.to_numeric(row,errors='coerce').isna()].duplicated().sum() > 1 ,axis=1)\n",
    "    clean_rows = df.loc[duplicate_idx].apply(remove_row_duplicates, axis=1).reset_index(drop=True)\n",
    "    j = 0\n",
    "    for i,flag in enumerate(duplicate_idx):\n",
    "        if not flag:\n",
    "            continue\n",
    "        df.iloc[i,:] = clean_rows.loc[j,:]\n",
    "        j += 1\n",
    "    df.replace([''],np.nan,regex=True,inplace=True) #':','$','%'\n",
    "    df.dropna(axis=1,how='all',inplace=True)\n",
    "    \n",
    "    columns = (~df.isna()).sum(axis=0) <= (4  if df.shape[0] > 12 else 2 if df.shape[0] == 4 else 0)\n",
    "    df = df.drop(columns=df.columns[columns])\n",
    "    return df.reset_index(drop=True),merge_pair_idxs\n",
    "\n",
    "\n",
    "\n",
    "def get_header_rows(\n",
    "    df_cur:pd.DataFrame,\n",
    ")->tuple:\n",
    "    for idx,row in df_cur.iterrows():\n",
    "        found = any(str(v).replace(\"$\",'').replace(\"%\",'').isnumeric() for v in row)\n",
    "        if found:\n",
    "            out = df_cur.iloc[:idx,:].apply(lambda row: ' '.join(row[row.notna()].values), axis=0)\n",
    "            return out\n",
    "    \n",
    "    return strip_string(df_cur.iloc[0].tolist())\n",
    "\n",
    "\n",
    "def main()->None:\n",
    "    qtrs = os.listdir('.')\n",
    "    ex = exceptions()\n",
    "    ex_rows = '|'.join(except_rows())\n",
    "    for qtr in qtrs:\n",
    "        if '.csv' in qtr or not os.path.exists(os.path.join(qtr,f'Schedule_of_Investments_0.csv')):\n",
    "            continue\n",
    "        qtr = '2020-12-31'\n",
    "        # logger.info(qtr)\n",
    "        display(qtr)\n",
    "        index_list_sum = i = 0\n",
    "        soi_files = sorted([\n",
    "            os.path.join(qtr,file) \n",
    "            for file in os.listdir(qtr)\n",
    "            if file.endswith('.csv')\n",
    "        ],key=lambda f: int(f.split('_')[-1].split('.')[0]))\n",
    "        # soi_files = [f for f in soi_files] # if f not in ex]\n",
    "        df,merged_pair_idxs = _clean(soi_files[i],except_rows=ex_rows,merged_pair_idxs={})\n",
    "\n",
    "        index_list = df.apply(\n",
    "            lambda row:row.astype(str).str.contains(stopping_criterion(qtr), case=False, na=False).any(),\n",
    "            axis=1\n",
    "        )\n",
    "        index_list_sum = index_list.sum()\n",
    "        dfs = [df]     \n",
    "        i += 1\n",
    "\n",
    "        while index_list_sum == 0:\n",
    "            # logger.info(soi_files[i])\n",
    "            merged_pair_idxs = ex.get(soi_files[i],merged_pair_idxs)\n",
    "            display(soi_files[i])\n",
    "\n",
    "            # display(merged_pair_idxs)\n",
    "            df,merged_pair_idxs = _clean(soi_files[i],except_rows=ex_rows,merged_pair_idxs=merged_pair_idxs if soi_files[i] not in ex else {})\n",
    "            dfs.append(df)\n",
    "            index_list = df.apply(\n",
    "                lambda row:row.astype(str).str.contains(stopping_criterion(qtr), case=False, na=False).any(),\n",
    "                axis=1\n",
    "            )\n",
    "            index_list_sum = index_list.sum()\n",
    "            i += 1\n",
    "        date_final = dfs[0]\n",
    "        if len(dfs) > 1:\n",
    "            date_final = pd.concat(dfs,axis=0,ignore_index=True)#pd.DataFrame(concat(*dfs))\n",
    "        # date_final = extract_subheaders(date_final,control=True)\n",
    "        # date_final = extract_subheaders(date_final,control=False)\n",
    "\n",
    "        date_final['qtr'] = qtr.split('\\\\')[-1]\n",
    "        if not os.path.exists(os.path.join(qtr,'output')):\n",
    "            os.makedirs(os.path.join(qtr,'output'))\n",
    "        columns_to_drop = date_final.notna().sum() <= 2\n",
    "        date_final.drop(columns=columns_to_drop[columns_to_drop].index)\n",
    "        date_final.to_csv(os.path.join(qtr,'output',f'{qtr}.csv'),index=False)\n",
    "        break\n",
    "    \n",
    "    # Use glob to find files\n",
    "    files = sorted(glob.glob(f'*/output/*.csv'), key=extract_date)\n",
    "    single_truth = pd.concat([\n",
    "        pd.read_csv(df) for df in files\n",
    "    ],axis=0,ignore_index=True)\n",
    "    single_truth.drop(columns=single_truth.columns[['Unnamed' in col for col in single_truth.columns]],inplace=True)\n",
    "    single_truth.to_csv(f'{cik}_soi_table.csv',index=False)\n",
    "    \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "cik = 1372807\n",
    "# logger = init_logger(cik)\n",
    "# logger.info(cik)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Portfolio_Company</th>\n",
       "      <th>Investment</th>\n",
       "      <th>Aquisition_Date</th>\n",
       "      <th>Principal/Shares</th>\n",
       "      <th>Cost</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PortfolioCompany/</td>\n",
       "      <td>Investment(15)</td>\n",
       "      <td>InitialAcquisitionDate</td>\n",
       "      <td>PercentageOwnership/Shares</td>\n",
       "      <td>Cost</td>\n",
       "      <td>FairValue(2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AdvancedLightingTechnologies,Inc,(8)(13)Consum...</td>\n",
       "      <td>Warrants</td>\n",
       "      <td>6/13/2012</td>\n",
       "      <td>1.90%</td>\n",
       "      <td>$ —</td>\n",
       "      <td>$ 1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdvancedLightingTechnologies,Inc.(8)(13)Consum...</td>\n",
       "      <td>MembershipInterests</td>\n",
       "      <td>6/13/2012</td>\n",
       "      <td>0.40%</td>\n",
       "      <td>181999</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AerostructuresHoldingsL.P.(8)(13)AerospaceandD...</td>\n",
       "      <td>PartnershipInterests</td>\n",
       "      <td>2/28/2007</td>\n",
       "      <td>1.16%</td>\n",
       "      <td>157717</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BCPGreatLakesFundLP(17)LimitedPartnership</td>\n",
       "      <td>PartnershipInterest</td>\n",
       "      <td>12/11/2018</td>\n",
       "      <td>55.6%</td>\n",
       "      <td>12466667</td>\n",
       "      <td>12466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CaribeMediaInc.(fkaCaribeInformationInvestment...</td>\n",
       "      <td>Common</td>\n",
       "      <td>12/18/2006</td>\n",
       "      <td>1.17%</td>\n",
       "      <td>359765</td>\n",
       "      <td>108675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>eInstructionAcquisition,LLC(8)(13)Services:Bus...</td>\n",
       "      <td>MembershipUnits</td>\n",
       "      <td>7/2/2007</td>\n",
       "      <td>1.10%</td>\n",
       "      <td>1079617</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FPWRCACoinvestmentFundVII,Ltd.(3)(13)CapitalEq...</td>\n",
       "      <td>ClassAShares</td>\n",
       "      <td>2/2/2007</td>\n",
       "      <td>0.41%</td>\n",
       "      <td>1500000</td>\n",
       "      <td>669000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NewMillenniumHoldco,Inc.(MillenniumHealth,LLC)...</td>\n",
       "      <td>Common</td>\n",
       "      <td>10/7/2014</td>\n",
       "      <td>0.20%</td>\n",
       "      <td>1953299</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PerseusHoldingCorp.(8)(13)Hotel,Gaming&amp;Leisure</td>\n",
       "      <td>Common</td>\n",
       "      <td>4/5/2007</td>\n",
       "      <td>0.19%</td>\n",
       "      <td>400000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RoscoeInvestors,LLC(8)(13)Healthcare&amp;Pharmaceu...</td>\n",
       "      <td>ClassAUnits</td>\n",
       "      <td>3/26/2014</td>\n",
       "      <td>1.56%</td>\n",
       "      <td>1000000</td>\n",
       "      <td>653000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>StaffordLogistics,Inc.(dbaCustomEcology,Inc.)(...</td>\n",
       "      <td>ClassBUnits</td>\n",
       "      <td>6/25/2013</td>\n",
       "      <td>1.56%</td>\n",
       "      <td>—</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>StaffordLogistics,Inc.(dbaCustomEcology,Inc.)(...</td>\n",
       "      <td>ClassBEquity</td>\n",
       "      <td>6/25/2013</td>\n",
       "      <td>1.56%</td>\n",
       "      <td>—</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>TankPartnersHoldings,LLC(8)(10)(13)Aerospacean...</td>\n",
       "      <td>Unit</td>\n",
       "      <td>8/28/2014</td>\n",
       "      <td>15.5%</td>\n",
       "      <td>980000</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>TankPartnersHoldings,LLC(8)(13)AerospaceandDef...</td>\n",
       "      <td>Warrants</td>\n",
       "      <td>8/28/2014</td>\n",
       "      <td>1.04%</td>\n",
       "      <td>185205</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>TRSOII,Inc.(8)(13)Energy:Oil&amp;Gas</td>\n",
       "      <td>CommonStock</td>\n",
       "      <td>12/24/2012</td>\n",
       "      <td>5.40%</td>\n",
       "      <td>1680161</td>\n",
       "      <td>548345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TotalInvestmentinEquitySecurities(9%ofnetasset...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$ 21944430</td>\n",
       "      <td>$ 14504687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Portfolio_Company            Investment  \\\n",
       "0                                   PortfolioCompany/        Investment(15)   \n",
       "1   AdvancedLightingTechnologies,Inc,(8)(13)Consum...              Warrants   \n",
       "2   AdvancedLightingTechnologies,Inc.(8)(13)Consum...   MembershipInterests   \n",
       "3   AerostructuresHoldingsL.P.(8)(13)AerospaceandD...  PartnershipInterests   \n",
       "4           BCPGreatLakesFundLP(17)LimitedPartnership   PartnershipInterest   \n",
       "5   CaribeMediaInc.(fkaCaribeInformationInvestment...                Common   \n",
       "6   eInstructionAcquisition,LLC(8)(13)Services:Bus...       MembershipUnits   \n",
       "7   FPWRCACoinvestmentFundVII,Ltd.(3)(13)CapitalEq...          ClassAShares   \n",
       "8   NewMillenniumHoldco,Inc.(MillenniumHealth,LLC)...                Common   \n",
       "9      PerseusHoldingCorp.(8)(13)Hotel,Gaming&Leisure                Common   \n",
       "10  RoscoeInvestors,LLC(8)(13)Healthcare&Pharmaceu...           ClassAUnits   \n",
       "11  StaffordLogistics,Inc.(dbaCustomEcology,Inc.)(...           ClassBUnits   \n",
       "12  StaffordLogistics,Inc.(dbaCustomEcology,Inc.)(...          ClassBEquity   \n",
       "13  TankPartnersHoldings,LLC(8)(10)(13)Aerospacean...                  Unit   \n",
       "14  TankPartnersHoldings,LLC(8)(13)AerospaceandDef...              Warrants   \n",
       "15                   TRSOII,Inc.(8)(13)Energy:Oil&Gas           CommonStock   \n",
       "16  TotalInvestmentinEquitySecurities(9%ofnetasset...                   NaN   \n",
       "17                                                NaN                   NaN   \n",
       "\n",
       "           Aquisition_Date            Principal/Shares         Cost  \\\n",
       "0   InitialAcquisitionDate  PercentageOwnership/Shares         Cost   \n",
       "1                6/13/2012                       1.90%          $ —   \n",
       "2                6/13/2012                       0.40%       181999   \n",
       "3                2/28/2007                       1.16%       157717   \n",
       "4               12/11/2018                       55.6%     12466667   \n",
       "5               12/18/2006                       1.17%       359765   \n",
       "6                 7/2/2007                       1.10%      1079617   \n",
       "7                 2/2/2007                       0.41%      1500000   \n",
       "8                10/7/2014                       0.20%      1953299   \n",
       "9                 4/5/2007                       0.19%       400000   \n",
       "10               3/26/2014                       1.56%      1000000   \n",
       "11               6/25/2013                       1.56%            —   \n",
       "12               6/25/2013                       1.56%            —   \n",
       "13               8/28/2014                       15.5%       980000   \n",
       "14               8/28/2014                       1.04%       185205   \n",
       "15              12/24/2012                       5.40%      1680161   \n",
       "16                     NaN                         NaN   $ 21944430   \n",
       "17                     NaN                         NaN          NaN   \n",
       "\n",
       "           Value  \n",
       "0   FairValue(2)  \n",
       "1         $ 1000  \n",
       "2           1000  \n",
       "3          50000  \n",
       "4       12466667  \n",
       "5         108675  \n",
       "6           1000  \n",
       "7         669000  \n",
       "8           1000  \n",
       "9           1000  \n",
       "10        653000  \n",
       "11          1000  \n",
       "12          1000  \n",
       "13          1000  \n",
       "14          1000  \n",
       "15        548345  \n",
       "16    $ 14504687  \n",
       "17           NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'Portfolio_Company': array([ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False]),\n",
       " '': array([ True,  True, False,  True,  True, False, False, False, False,\n",
       "         True,  True, False, False, False, False,  True,  True, False,\n",
       "        False, False, False,  True,  True, False, False, False, False,\n",
       "         True, False]),\n",
       " 'Investment': array([ True, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False]),\n",
       " 'Aquisition_Date': array([ True,  True,  True,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False]),\n",
       " 'Principal/Shares': array([ True,  True,  True,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False]),\n",
       " 'Cost': array([ True,  True,  True,  True, False, False, False, False, False,\n",
       "        False, False, False, False]),\n",
       " 'Value': array([ True,  True,  True,  True, False, False, False, False, False,\n",
       "        False])}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = r'2018-12-31\\\\Schedule_of_Investments_4.csv'\n",
    "\n",
    "\n",
    "\n",
    "def standard_field_names()->tuple:\n",
    "    return (\n",
    "        'Portfolio Company',\n",
    "        # 'Portfolio Company /Principal Business',\n",
    "        'Percentage  Interest/  Shares',\n",
    "        'Investment /Interest Rate /Maturity',        \n",
    "        'Principal',\n",
    "        'Cost',\n",
    "        'Value',\n",
    "        'Percent of Class Held',\n",
    "        'Investment',\n",
    "        'CDO Fund Investments',\n",
    "        'Percent of Interests Held',\n",
    "        'Industry',\n",
    "        'Spread Above Index',\n",
    "        'Aquisition Date',\n",
    "        'Interest Rate',\n",
    "        'Maturity',\n",
    "        'Principal/Shares',\n",
    "        'Investment Type',\n",
    "        'of Net Assets',\n",
    "        'business description',\n",
    "        'type of investment',\n",
    "        'investment date',\n",
    "        'reference rate and spread',\n",
    "        'pik rate',\n",
    "        'maturity date',\n",
    "        'cost',\n",
    "        'footnotes',\n",
    "        'industry',\n",
    "        'principal amount', # TODO change stand names for more dynamic fuzzywuzzy matching\n",
    "        'fair value',\n",
    "    )\n",
    "\n",
    "def merge_duplicate_columns(\n",
    "    df:pd.DataFrame,\n",
    "    merged_pair_idxs:dict={}\n",
    ")->pd.DataFrame:\n",
    "    duplicate_cols = merged_pair_idxs.keys()\n",
    "    flag = not merged_pair_idxs.keys()\n",
    "    if flag: \n",
    "        duplicate_cols = df.columns.unique() \n",
    "    for col_name in duplicate_cols:\n",
    "        # display(col_name)\n",
    "        mask = merged_pair_idxs.get(col_name)\n",
    "        if flag:\n",
    "            mask = df.columns == col_name\n",
    "            merged_pair_idxs[col_name] = mask\n",
    "        duplicate_data = df.loc[:, mask]\n",
    "        merged_data = duplicate_data.apply(lambda row: ' '.join(set(row.dropna().astype(str))), axis=1)\n",
    "        df = df.loc[:, ~mask]\n",
    "        df[col_name] = merged_data\n",
    "        # display(df)\n",
    "    return df.reset_index(drop=True),merged_pair_idxs\n",
    "\n",
    "def _clean(\n",
    "    file_path:str,\n",
    "    except_rows:str,\n",
    "    merged_pair_idxs:dict={},\n",
    ")->pd.DataFrame:\n",
    "    df = pd.read_csv(file_path,index_col=0,na_values=[' ', ''])\n",
    "    # df.replace(['\\xa0','\\u200b',r'^\\s$',r'^\\s%'],np.nan,regex=True,inplace=True) #':','$','%' r'^\\s*$',r'^\\s*%'\n",
    "    df.replace(['\\xa0','\\u200b',r'^\\s*$',r'^\\s*%',' ','˄'],'',regex=True,inplace=True)\n",
    "    df.replace(['PrincipalBusiness'],'',regex=True,inplace=True)\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    df.dropna(axis=0,how='all',inplace=True)\n",
    "    df = df[~df.apply(lambda row:row.astype(str).str.contains(except_rows,case=False, na=False).any(),axis=1)]\n",
    "    if df.shape[0] < 3:\n",
    "        return pd.DataFrame(), merged_pair_idxs\n",
    "    if not merged_pair_idxs:\n",
    "        important_fields = strip_string(get_header_rows(df),standardize=True)#get_key_fields(df)\n",
    "        df.columns = important_fields\n",
    "    df,merge_pair_idxs = merge_duplicate_columns(df,merged_pair_idxs=merged_pair_idxs)\n",
    "    duplicate_idx = df.apply(lambda row:row[pd.to_numeric(row,errors='coerce').isna()].duplicated().sum() > 1 ,axis=1)\n",
    "    clean_rows = df.loc[duplicate_idx].apply(remove_row_duplicates, axis=1).reset_index(drop=True)\n",
    "    j = 0\n",
    "    for i,flag in enumerate(duplicate_idx):\n",
    "        if not flag:\n",
    "            continue\n",
    "        df.iloc[i,:] = clean_rows.loc[j,:]\n",
    "        j += 1\n",
    "    df.replace([''],np.nan,regex=True,inplace=True) #':','$','%'\n",
    "    df.dropna(axis=1,how='all',inplace=True)\n",
    "    \n",
    "    columns = (~df.isna()).sum(axis=0) <= (4  if df.shape[0] > 12 else 2 if df.shape[0] == 4 else 0)\n",
    "    df = df.drop(columns=df.columns[columns])\n",
    "    return df.reset_index(drop=True),merge_pair_idxs\n",
    "\n",
    "\n",
    "merged_pair_idxs = {}\n",
    "ex = exceptions()\n",
    "ex_rows = '|'.join(except_rows())\n",
    "df,merged_pair_idxs = _clean(file_path,except_rows=ex_rows,merged_pair_idxs=merged_pair_idxs)\n",
    "display(df)\n",
    "display(merged_pair_idxs)\n",
    "index_list = df.apply(\n",
    "    lambda row:row.astype(str).str.contains(stopping_criterion(None), case=False, na=False).any(),\n",
    "    axis=1\n",
    ")\n",
    "index_list_sum = index_list.sum()\n",
    "index_list_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  0\n",
       "0           0  5"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('2015-03-31\\\\Schedule_of_Investments_1.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sec_windows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
