{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/Desktop/My_repos/sec_filings/sec/lib/python3.7/site-packages/fuzzywuzzy/fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2010-09-30'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "WARNING:root:Applied processor reduces input query to empty string, all comparisons will have score 0. [Query: '']\n",
      "/home/tony/Desktop/My_repos/sec_filings/sec/lib/python3.7/site-packages/ipykernel_launcher.py:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/home/tony/Desktop/My_repos/sec_filings/sec/lib/python3.7/site-packages/ipykernel_launcher.py:228: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2010-09-30/Schedule_of_Investments_1.csv'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27518/254325877.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    448\u001b[0m     }\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_27518/254325877.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mindex_list_sum\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;31m# logger.info(soi_files[i])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0mmerged_pair_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoi_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmerged_pair_idxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoi_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import datetime\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "sys.path.insert(0, '../') \n",
    "# from utils import init_logger\n",
    "\n",
    "def common_subheaders()->tuple:\n",
    "    return tuple(map(lambda header:header.replace(' ', r'\\s*'),\n",
    "        ('Advertising, Public Relations and Marketing ',\n",
    "        'Air Transportation',\n",
    "        'Amusement and Recreation',\n",
    "        'Apparel Manufacturing',\n",
    "        'Building Equipment Contractors',\n",
    "        'Business Support Services',\n",
    "        'Chemicals',\n",
    "        'Communications Equipment Manufacturing',\n",
    "        'Credit Related Activities',\n",
    "        'Computer Systems Design and Related Services',\n",
    "        'Credit (Nondepository)',\n",
    "        'Data Processing and Hosting Services',\n",
    "        'Educational Support Services',\n",
    "        'Electronic Component Manufacturing',\n",
    "        'Equipment Leasing',\n",
    "        'Facilities Support Services',\n",
    "        'Grocery Stores',\n",
    "        'Hospitals',\n",
    "        'Insurance',\n",
    "        'Lessors of Nonfinancial Licenses',\n",
    "        'Management, Scientific, and Technical Consulting Services',\n",
    "        'Motion Picture and Video Industries',\n",
    "        'Other Information Services',\n",
    "        'Other Manufacturing',\n",
    "        'Other Publishing',\n",
    "        'Other Real Estate Activities',\n",
    "        'Other Telecommunications',\n",
    "        'Plastics Manufacturing',\n",
    "        'Radio and Television Broadcasting',\n",
    "        'Real Estate Leasing',\n",
    "        'Restaurants',\n",
    "        'Retail',\n",
    "        'Satellite Telecommunications',\n",
    "        'Scientific Research and Development Services',\n",
    "        'Texttile Furnishings Mills',\n",
    "        'Traveler Arrangement',\n",
    "        'Software Publishing',\n",
    "        'Utility System Construction',\n",
    "        'Wholesalers',\n",
    "        'Wired Telecommunications Carriers',\n",
    "        'Wireless Telecommunications Carriers',\n",
    "        )\n",
    "    ))\n",
    "\n",
    "\n",
    "def standard_field_names()->tuple:\n",
    "    return (\n",
    "        'Portfolio Company',\n",
    "        # 'Portfolio Company /Principal Business',\n",
    "        'Investment /Interest Rate /Maturity',\n",
    "        'Percentage  Interest/  Shares',\n",
    "        'Principal',\n",
    "        'Cost',\n",
    "        'Value',\n",
    "        'Percentage Ownership',\n",
    "        'Percent of Class Held',\n",
    "        # 'Investment',\n",
    "        'CDO Fund Investments',\n",
    "        'Percent of Interests Held',\n",
    "        # 'Industry',\n",
    "        'Spread Above Index',\n",
    "        'Aquisition Date',\n",
    "        # 'Maturity',\n",
    "        # 'Principal/Shares',\n",
    "        # 'Investment Type',\n",
    "        'of Net Assets',\n",
    "        # 'business description',\n",
    "        # 'type of investment',\n",
    "        # 'investment date',\n",
    "        'reference rate and spread',\n",
    "        'pik rate',\n",
    "        # 'maturity date',\n",
    "        # 'cost',\n",
    "        'footnotes',\n",
    "        # 'industry',\n",
    "        # 'principal amount',\n",
    "        # 'fair value',\n",
    "    )\n",
    "\n",
    "    \n",
    "def company_control_headers()->tuple:\n",
    "    return tuple(map(lambda header:header.replace(' ', r'\\s*'),\n",
    "        (\n",
    "        'Debt Investments',\n",
    "        'Debt Investments (82.23%)',\n",
    "        'Debt Investments (A)',\n",
    "        'Debt Investments (continued)',\n",
    "        'Equity Securities',\n",
    "        'Equity Securities (continued)',\n",
    "        'Cash and Cash Equivalents',\n",
    "        )\n",
    "    ))\n",
    "\n",
    "\n",
    "def except_rows()->tuple:\n",
    "    return (\n",
    "        'asdf',\n",
    "    )\n",
    "\n",
    "# https://www.sec.gov/robots.txt\n",
    "def get_standard_name(col, choices, score_cutoff=60):\n",
    "    best_match, score = process.extractOne(col, choices)\n",
    "    if score > score_cutoff:\n",
    "        return best_match\n",
    "    return col\n",
    "\n",
    "def stopping_criterion(qtr:str)->str:\n",
    "    return '{}'.format(r'Total_*Investments')\n",
    "\n",
    " \n",
    "def concat(*dfs)->list:\n",
    "    final = []\n",
    "    for df in dfs:\n",
    "        final.extend(df.values.tolist())\n",
    "    return final\n",
    "\n",
    "    \n",
    "def get_key_fields(\n",
    "    df_cur:pd.DataFrame,\n",
    ")->tuple:\n",
    "    important_fields = standard_field_names() + common_subheaders()\n",
    "    for idx,row in enumerate(df_cur.iterrows()):\n",
    "        found = any(any(\n",
    "            key in str(field).lower() \n",
    "            for key in important_fields)\n",
    "                    for field in row[-1].dropna().tolist()\n",
    "            )\n",
    "        if found and len(set(row[-1].dropna().tolist())) >= 6:\n",
    "            cols = df_cur.iloc[:idx + 1].apply(lambda row: ' '.join(row.dropna()), axis=0).tolist()\n",
    "            fields = strip_string(cols,standardize=found) \n",
    "            return fields\n",
    "    return strip_string(df_cur.iloc[0].tolist())\n",
    "\n",
    "def strip_string(\n",
    "    columns_names:list,\n",
    "    standardize:bool=False\n",
    ")->tuple:\n",
    "    # columns = tuple(map(lambda col:re.sub(r'[^a-z]', '', str(col).lower()),columns_names))\n",
    "    if standardize:\n",
    "        standard_fields = standard_field_names()\n",
    "        return tuple(\n",
    "            re.sub(r'\\s+', '_',get_standard_name(str(col),standard_fields)) for col in columns_names\n",
    "        )\n",
    "    return tuple(re.sub(r'\\s+', '_',str(col)) for col in columns_names)\n",
    "\n",
    "\n",
    "# Function to extract date and convert to datetime object\n",
    "def extract_date(file_path):\n",
    "    # Extract date from file path (assuming date is always in 'YYYY-MM-DD' format)\n",
    "    date_str = re.search(r'\\d{4}-\\d{2}-\\d{2}', file_path).group()\n",
    "    return datetime.datetime.strptime(date_str, '%Y-%m-%d')\n",
    "\n",
    "\n",
    "def merge_duplicate_columns(\n",
    "    df:pd.DataFrame,\n",
    "    merged_pair_idxs:dict={}\n",
    ")->pd.DataFrame:\n",
    "    duplicate_cols = merged_pair_idxs.keys()\n",
    "    flag = not merged_pair_idxs.keys()\n",
    "    if flag: \n",
    "        duplicate_cols = df.columns.unique() \n",
    "    for col_name in duplicate_cols:\n",
    "        # display(col_name)\n",
    "        mask = merged_pair_idxs.get(col_name)\n",
    "        if flag:\n",
    "            mask = df.columns == col_name\n",
    "            merged_pair_idxs[col_name] = mask\n",
    "        duplicate_data = df.loc[:, mask]\n",
    "        merged_data = duplicate_data.apply(lambda row: ' '.join(set(row.dropna().astype(str))), axis=1)\n",
    "        df = df.loc[:, ~mask]\n",
    "        df[col_name] = merged_data\n",
    "        # display(df)\n",
    "    return df.reset_index(drop=True),merged_pair_idxs\n",
    "\n",
    "def extract_subheaders(\n",
    "    df:pd.DataFrame,\n",
    "    control:bool,\n",
    ")->pd.DataFrame:\n",
    "    col_name = 'company_control' if control else 'Type_of_Investment'\n",
    "    if col_name in df.columns:\n",
    "        return df\n",
    "    include = df.apply(\n",
    "        lambda row: re.search('|'.join(company_control_headers() if control else common_subheaders()), str(row[0]), re.IGNORECASE) is not None,\n",
    "        axis=1\n",
    "    )  \n",
    "    \n",
    "    exclude = ~df.apply(\n",
    "        lambda row: row.astype(str).str.contains('total|Inc|Ltd|LLC|Holdings|LP|Co|Corporation', case=False, na=False).any(),\n",
    "        axis=1\n",
    "    )\n",
    "    idx = df[include & exclude].index.tolist()\n",
    "    df[col_name] = None\n",
    "    if not idx:\n",
    "        return df\n",
    "\n",
    "    prev_header = subheader = None\n",
    "    df.loc[idx[-1]:,col_name] = df.iloc[idx[-1],1] if isinstance(df.iloc[idx[-1],0],float)  else df.iloc[idx[-1],0]\n",
    "    for j,i in enumerate(idx[:-1]):\n",
    "        prev_header = subheader\n",
    "        subheader = df.iloc[i,1] if isinstance(df.iloc[i,0],float)  else df.iloc[i,0]\n",
    "        df.loc[idx[j]:idx[j+1],col_name] = subheader if subheader != '' else prev_header\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_row_duplicates(row:pd.Series)->pd.Series: \n",
    "    out = []\n",
    "    for v in row:\n",
    "        if v in out and not str(v).replace('$','').isnumeric():\n",
    "            out.append(np.nan)\n",
    "        else:\n",
    "            out.append(v)\n",
    "    return pd.Series(out)\n",
    "\n",
    "  \n",
    "\n",
    "def _clean(\n",
    "    file_path:str,\n",
    "    except_rows:str,\n",
    "    merged_pair_idxs:dict={},\n",
    ")->pd.DataFrame:\n",
    "    df = pd.read_csv(file_path,index_col=0,na_values=[' ', ''])\n",
    "    df.replace(to_replace=r'[\\[\\](){},$%˄\\xa0\\u200b]', value='', regex=True,inplace=True)\n",
    "    df.replace(['PrincipalBusiness',' '],'_',regex=True,inplace=True)\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    df.dropna(axis=0,how='all',inplace=True)\n",
    "    df = df[~df.apply(lambda row:row.astype(str).str.contains(except_rows,case=False, na=False).any(),axis=1)]\n",
    "    # display(df)\n",
    "    if not df.apply(lambda col: col.astype(str).str.contains(r'total_*investments', case=False, regex=True)).any().any() and df.shape[0] < 3:\n",
    "        return pd.DataFrame(), merged_pair_idxs\n",
    "    \n",
    "    if not merged_pair_idxs:\n",
    "        important_fields = strip_string(get_header_rows(df),standardize=True)#get_key_fields(df)\n",
    "        df.columns = important_fields\n",
    "    df,merge_pair_idxs = merge_duplicate_columns(df,merged_pair_idxs=merged_pair_idxs)\n",
    "    duplicate_idx = df.apply(lambda row:row[pd.to_numeric(row,errors='coerce').isna()].duplicated().sum() > 1 ,axis=1)\n",
    "    clean_rows = df.loc[duplicate_idx].apply(remove_row_duplicates, axis=1).reset_index(drop=True)\n",
    "    j = 0\n",
    "    for i,flag in enumerate(duplicate_idx):\n",
    "        if not flag:\n",
    "            continue\n",
    "        df.iloc[i,:] = clean_rows.loc[j,:]\n",
    "        j += 1\n",
    "    df.replace([''],np.nan,regex=True,inplace=True) #':','$','%'\n",
    "    df.dropna(axis=1,how='all',inplace=True)\n",
    "    \n",
    "    columns = (~df.isna()).sum(axis=0) < (4  if df.shape[0] > 12 else 2 if df.shape[0] == 4 else 0)\n",
    "    df = df.drop(columns=df.columns[columns])\n",
    "    return df.reset_index(drop=True),merge_pair_idxs\n",
    "\n",
    "\n",
    "\n",
    "def get_header_rows(\n",
    "    df_cur:pd.DataFrame,\n",
    ")->tuple:\n",
    "    for idx,row in df_cur.iterrows():\n",
    "        found = any(str(v).replace(\"$\",'').replace(\"%\",'').isnumeric() for v in row)\n",
    "        if found:\n",
    "            out = df_cur.iloc[:idx,:].apply(lambda row: ' '.join(row[row.notna()].values), axis=0)\n",
    "            return out\n",
    "    \n",
    "    return strip_string(df_cur.iloc[0].tolist())\n",
    "\n",
    "\n",
    "def main()->None:\n",
    "    qtrs = os.listdir('.')\n",
    "    ex = exceptions()\n",
    "    ex_rows = '|'.join(except_rows())\n",
    "    for qtr in qtrs:\n",
    "        if '.csv' in qtr or not os.path.exists(os.path.join(qtr,f'Schedule_of_Investments_0.csv')):\n",
    "            continue\n",
    "        # qtr = '2008-12-31'\n",
    "        # logger.info(qtr)\n",
    "        display(qtr)\n",
    "        index_list_sum = i = 0\n",
    "        soi_files = sorted([\n",
    "            os.path.join(qtr,file) \n",
    "            for file in os.listdir(qtr)\n",
    "            if file.endswith('.csv')\n",
    "        ],key=lambda f: int(f.split('_')[-1].split('.')[0]))\n",
    "        # soi_files = [f for f in soi_files] # if f not in ex]\n",
    "        if len(soi_files) == 0:\n",
    "            continue\n",
    "        df,merged_pair_idxs = _clean(soi_files[i],except_rows=ex_rows,merged_pair_idxs={})\n",
    "\n",
    "        index_list = df.apply(\n",
    "            lambda row:row.astype(str).str.contains(stopping_criterion(qtr), case=False, na=False).any(),\n",
    "            axis=1\n",
    "        )\n",
    "        index_list_sum = index_list.sum()\n",
    "        dfs = [df]     \n",
    "        i += 1\n",
    "\n",
    "        while index_list_sum == 0:\n",
    "            # logger.info(soi_files[i])\n",
    "            merged_pair_idxs = ex.get(soi_files[i],merged_pair_idxs)\n",
    "            display(soi_files[i])\n",
    "\n",
    "            # display(merged_pair_idxs)\n",
    "            df,merged_pair_idxs = _clean(soi_files[i],except_rows=ex_rows,merged_pair_idxs=merged_pair_idxs if soi_files[i] not in ex else {})\n",
    "            dfs.append(df)\n",
    "            index_list = df.apply(\n",
    "                lambda row:row.astype(str).str.contains(stopping_criterion(qtr), case=False, na=False).any(),\n",
    "                axis=1\n",
    "            )\n",
    "            index_list_sum = index_list.sum()\n",
    "            i += 1\n",
    "        date_final = dfs[0]\n",
    "        if len(dfs) > 1:\n",
    "            date_final = pd.concat(dfs,axis=0,ignore_index=True)#pd.DataFrame(concat(*dfs))\n",
    "        # date_final = extract_subheaders(date_final,control=True)\n",
    "        # date_final = extract_subheaders(date_final,control=False)\n",
    "\n",
    "        date_final['qtr'] = qtr.split('/')[-1]\n",
    "        if not os.path.exists(os.path.join(qtr,'output')):\n",
    "            os.makedirs(os.path.join(qtr,'output'))\n",
    "        columns_to_drop = date_final.notna().sum() <= 2\n",
    "        date_final.drop(columns=columns_to_drop[columns_to_drop].index)\n",
    "        date_final.to_csv(os.path.join(qtr,'output',f'{qtr}.csv'),index=False)\n",
    "        # break\n",
    "    \n",
    "    # Use glob to find files\n",
    "    files = sorted(glob.glob(f'*/output/*.csv'), key=extract_date)\n",
    "    single_truth = pd.concat([\n",
    "        pd.read_csv(df) for df in files\n",
    "    ],axis=0,ignore_index=True)\n",
    "    single_truth.drop(columns=single_truth.columns[['Unnamed' in col for col in single_truth.columns]],inplace=True)\n",
    "    single_truth.to_csv(f'{cik}_soi_table.csv',index=False)\n",
    "    \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "cik = 1372807\n",
    "# logger = init_logger(cik)\n",
    "# logger.info(cik)\n",
    "def exceptions()->dict:\n",
    "    return {\n",
    "        '2006-12-31/Schedule_of_Investments_1.csv':dict(),\n",
    "        '2006-12-31/Schedule_of_Investments_3.csv':dict(), \n",
    "        '2008-03-31/Schedule_of_Investments_7.csv':dict(),\n",
    "        '2008-03-31/Schedule_of_Investments_8.csv':dict(),\n",
    "        '2008-12-31/Schedule_of_Investments_6.csv':dict(),\n",
    "        '2008-12-31/Schedule_of_Investments_11.csv':dict(),\n",
    "        '2008-12-31/Schedule_of_Investments_13.csv':dict(),\n",
    "        '2008-12-31/Schedule_of_Investments_14.csv': {\n",
    "                    'Portfolio_Company_/Principal_Business': np.array([ True, False, False, False, False, False, False, False, False,False, False, False, False, False,False]),\n",
    "                    '': np.array([ True, False,  True, True, False, True,  True, True, False,True,  True, True, False, True,False]),\n",
    "                    'Investment': np.array([ True, False, False, False, False, False]),\n",
    "                    'Percent_of_Interests_Held': np.array([ True,  False,  False, False, False, False]),\n",
    "                    'Cost': np.array([True,  False, False, False, False, False]),\n",
    "                    'Value': np.array([True,  False, False, False, False, False])\n",
    "                },\n",
    "        '2009-03-31/Schedule_of_Investments_2.csv':dict(),\n",
    "        '2009-03-31/Schedule_of_Investments_5.csv':dict(),\n",
    "        '2009-03-31/Schedule_of_Investments_6.csv':dict(),\n",
    "        '2009-03-31/Schedule_of_Investments_9.csv':dict(),\n",
    "        '2009-09-30/Schedule_of_Investments_8.csv':dict(),\n",
    "        '2009-12-31/Schedule_of_Investments_6.csv':dict(),\n",
    "        '2009-12-31/Schedule_of_Investments_7.csv':dict(),\n",
    "        '2009-12-31/Schedule_of_Investments_7.csv':dict(),\n",
    "        '2009-12-31/Schedule_of_Investments_12.csv': {\n",
    "            'Portfolio Company': np.array([ True, False, False, False, False, False, False, False, False,False, False, False, False, False]),\n",
    "            'Investment /Interest Rate /Maturity': np.array([ False, True, False, False, False, False, False, False, False,False, False, False, False, False]),\n",
    "            'Percent_of_Interests_Held':np.array([False,False,True,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "            'Cost':np.array([False,False,False,False,True,True,False,False,False,False,False,False,False,False]),\n",
    "            'Value':np.array([False,False,False,False,False,False,True,True,False,False,False,False,False]),\n",
    "            '':np.array([True]*6 + [False]*6),\n",
    "        },\n",
    "        '2010-03-31/Schedule_of_Investments_7.csv':dict(),\n",
    "        '2010-03-31/Schedule_of_Investments_8.csv':dict(),\n",
    "        '2010-06-30/Schedule_of_Investments_19.csv':dict(),\n",
    "        '2011-06-30/Schedule_of_Investments_4.csv':dict(),\n",
    "        '2011-06-30/Schedule_of_Investments_6.csv':dict(),\n",
    "        '2011-06-30/Schedule_of_Investments_9.csv':dict(),\n",
    "        '2011-12-31/Schedule_of_Investments_9.csv':dict(),\n",
    "        '2012-03-31/Schedule_of_Investments_27.csv':dict(),\n",
    "        '2018-09-30/Schedule_of_Investments_2.csv':dict(),\n",
    "        '2019-03-31/Schedule_of_Investments_14.csv':dict(),\n",
    "        '2020-12-31/Schedule_of_Investments_16.csv':{\n",
    "            'Investment':np.array([True,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "            'Cost':np.array([False,False,False,False,False,False,False,False,True,False,False,False,False,False,False]),\n",
    "            'Value':np.array([False,False,False,False,False,False,False,False,False,False,False,True,False,False,False]),\n",
    "            '':np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "        },\n",
    "        '2021-03-31/Schedule_of_Investments_16.csv': {\n",
    "            'Investment':np.array([True,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "            'Cost':np.array([False,False,False,False,False,False,False,False,True,False,False,False,False,False,False]),\n",
    "            'Value':np.array([False,False,False,False,False,False,False,False,False,False,False,True,False,False,False]),\n",
    "            '':np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "        },\n",
    "        '2021-06-30/Schedule_of_Investments_16.csv':{\n",
    "            'Investment':np.array([True,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "            'Cost':np.array([False,False,False,False,False,False,False,False,True,False,False,False,False,False,False]),\n",
    "            'Value':np.array([False,False,False,False,False,False,False,False,False,False,False,True,False,False,False]),\n",
    "            '':np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "        },\n",
    "        '2021-09-30/Schedule_of_Investments_12.csv':dict(),\n",
    "        '2021-09-30/Schedule_of_Investments_13.csv':dict(),\n",
    "        '2021-09-30/Schedule_of_Investments_14.csv': {\n",
    "            'Investment':np.array([True,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "            'Cost':np.array([False,False,False,False,False,False,False,False,True,False,False,False,False,False,False]),\n",
    "            'Value':np.array([False,False,False,False,False,False,False,False,False,False,False,True,False,False,False]),\n",
    "            '':np.array([False,False,False,False,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "        },\n",
    "        '2022-03-31/Schedule_of_Investments_5.csv':dict(),\n",
    "        '2022-03-31/Schedule_of_Investments_6.csv':dict(),\n",
    "        '2022-06-30/Schedule_of_Investments_4.csv':dict(),\n",
    "        '2022-06-30/Schedule_of_Investments_6.csv':dict(),\n",
    "        '2022-06-30/Schedule_of_Investments_7.csv':dict(),\n",
    "        '2022-09-30/Schedule_of_Investments_4.csv':dict(),\n",
    "        '2022-09-30/Schedule_of_Investments_5.csv':dict(),\n",
    "        '2022-09-30/Schedule_of_Investments_6.csv':dict(),\n",
    "        '2022-09-30/Schedule_of_Investments_9.csv':dict(),\n",
    "        '2022-09-30/Schedule_of_Investments_14.csv':dict(),\n",
    "        '2022-12-31/Schedule_of_Investments_4.csv':dict(),\n",
    "        '2022-12-31/Schedule_of_Investments_5.csv':dict(),\n",
    "        '2022-12-31/Schedule_of_Investments_6.csv':dict(),\n",
    "        '2023-03-31/Schedule_of_Investments_4.csv':dict(),\n",
    "        '2023-03-31/Schedule_of_Investments_5.csv':dict(),\n",
    "        '2023-03-31/Schedule_of_Investments_6.csv':dict(),\n",
    "        '2023-06-30/Schedule_of_Investments_4.csv':dict(),\n",
    "        '2023-06-30/Schedule_of_Investments_5.csv':dict(),\n",
    "        '2023-06-30/Schedule_of_Investments_6.csv':dict(),\n",
    "        '2023-09-30/Schedule_of_Investments_4.csv':dict(),\n",
    "        '2023-09-30/Schedule_of_Investments_5.csv':dict(),\n",
    "        '2023-09-30/Schedule_of_Investments_6.csv':dict(),\n",
    "        '2023-12-31/Schedule_of_Investments_4.csv':dict(),\n",
    "        '2023-12-31/Schedule_of_Investments_5.csv':dict(),\n",
    "        '2023-12-31/Schedule_of_Investments_6.csv':dict(),\n",
    "        '2024-03-31/Schedule_of_Investments_1.csv':dict(),\n",
    "        '2024-03-31/Schedule_of_Investments_4.csv':dict(),\n",
    "        '2024-03-31/Schedule_of_Investments_5.csv':dict(),\n",
    "        '2024-03-31/Schedule_of_Investments_6.csv':dict(),\n",
    "        '2009-12-31/Schedule_of_Investments_6.csv':dict(),\n",
    "    }\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27518/3304876222.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m def merge_duplicate_columns(\n\u001b[1;32m     43\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mmerged_pair_idxs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m )->pd.DataFrame:\n\u001b[1;32m     46\u001b[0m     \u001b[0mduplicate_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerged_pair_idxs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "file_path = r'2009-12-31/Schedule_of_Investments_12.csv'\n",
    "\n",
    "\n",
    "\n",
    "def standard_field_names()->tuple:\n",
    "    return (\n",
    "        'Portfolio Company',\n",
    "        # 'Portfolio Company /Principal Business',\n",
    "        'Percentage  Interest/  Shares',\n",
    "        'Investment /Interest Rate /Maturity',        \n",
    "        'Principal',\n",
    "        'Cost',\n",
    "        'Value',\n",
    "        'Time Deposits and Money Market Account',\n",
    "        'Percentage Ownership',\n",
    "        'Initial  Acquisition  Date',\n",
    "        'Percent of Class Held',\n",
    "        'Investment',\n",
    "        'CDO Fund Investments',\n",
    "        'Percent of Interests Held',\n",
    "        'Industry',\n",
    "        'Spread Above Index',\n",
    "        'Aquisition Date',\n",
    "        'Interest Rate',\n",
    "        'Maturity',\n",
    "        'Principal/Shares',\n",
    "        'Investment Type',\n",
    "        'of Net Assets',\n",
    "        'business description',\n",
    "        'type of investment',\n",
    "        'investment date',\n",
    "        'reference rate and spread',\n",
    "        'pik rate',\n",
    "        'maturity date',\n",
    "        'cost',\n",
    "        'footnotes',\n",
    "        'industry',\n",
    "        'principal amount', # TODO change stand names for more dynamic fuzzywuzzy matching\n",
    "        'fair value',\n",
    "    )\n",
    "\n",
    "def merge_duplicate_columns(\n",
    "    df:pd.DataFrame,\n",
    "    merged_pair_idxs:dict={}\n",
    ")->pd.DataFrame:\n",
    "    duplicate_cols = merged_pair_idxs.keys()\n",
    "    flag = not merged_pair_idxs.keys()\n",
    "    if flag: \n",
    "        duplicate_cols = df.columns.unique() \n",
    "    for col_name in duplicate_cols:\n",
    "        # display(col_name)\n",
    "        mask = merged_pair_idxs.get(col_name)\n",
    "        if flag:\n",
    "            mask = df.columns == col_name\n",
    "            merged_pair_idxs[col_name] = mask\n",
    "        duplicate_data = df.loc[:, mask]\n",
    "        merged_data = duplicate_data.apply(lambda row: ' '.join(set(row.dropna().astype(str))), axis=1)\n",
    "        df = df.loc[:, ~mask]\n",
    "        df[col_name] = merged_data\n",
    "        # display(df)\n",
    "    return df.reset_index(drop=True),merged_pair_idxs\n",
    "\n",
    "def _clean(\n",
    "    file_path:str,\n",
    "    except_rows:str,\n",
    "    merged_pair_idxs:dict={},\n",
    ")->pd.DataFrame:\n",
    "    df = pd.read_csv(file_path,index_col=0,na_values=[' ', ''])\n",
    "    df.replace(to_replace=r'[\\[\\](){},$%˄\\xa0\\u200b]', value='', regex=True,inplace=True)\n",
    "    df.replace(['PrincipalBusiness',' '],'_',regex=True,inplace=True)\n",
    "    df = df.applymap(lambda x: x.strip() if isinstance(x, str) else x)\n",
    "    df.dropna(axis=0,how='all',inplace=True)\n",
    "    df = df[~df.apply(lambda row:row.astype(str).str.contains(except_rows,case=False, na=False).any(),axis=1)]\n",
    "    # display(df)\n",
    "    if not df.apply(lambda col: col.astype(str).str.contains(r'total_*investments', case=False, regex=True)).any().any() and df.shape[0] < 3:\n",
    "        return pd.DataFrame(), merged_pair_idxs\n",
    "    \n",
    "    if not merged_pair_idxs:\n",
    "        important_fields = strip_string(get_header_rows(df),standardize=True)#get_key_fields(df)\n",
    "        df.columns = important_fields\n",
    "    df,merge_pair_idxs = merge_duplicate_columns(df,merged_pair_idxs=merged_pair_idxs)\n",
    "    duplicate_idx = df.apply(lambda row:row[pd.to_numeric(row,errors='coerce').isna()].duplicated().sum() > 1 ,axis=1)\n",
    "    clean_rows = df.loc[duplicate_idx].apply(remove_row_duplicates, axis=1).reset_index(drop=True)\n",
    "    j = 0\n",
    "    for i,flag in enumerate(duplicate_idx):\n",
    "        if not flag:\n",
    "            continue\n",
    "        df.iloc[i,:] = clean_rows.loc[j,:]\n",
    "        j += 1\n",
    "    df.replace([''],np.nan,regex=True,inplace=True) #':','$','%'\n",
    "    df.dropna(axis=1,how='all',inplace=True)\n",
    "    \n",
    "    columns = (~df.isna()).sum(axis=0) < (4  if df.shape[0] > 12 else 2 if df.shape[0] == 4 else 0)\n",
    "    df = df.drop(columns=df.columns[columns])\n",
    "    return df.reset_index(drop=True),merge_pair_idxs\n",
    "\n",
    "\n",
    "\n",
    "merged_pair_idxs = {\n",
    "            'Portfolio Company': np.array([ True, False, False, False, False, False, False, False, False,False, False, False, False, False]),\n",
    "            'Investment /Interest Rate /Maturity': np.array([ False, True, False, False, False, False, False, False, False,False, False, False, False, False]),\n",
    "            'Percent_of_Interests_Held':np.array([False,False,True,False,False,False,False,False,False,False,False,False,False,False]),\n",
    "            'Cost':np.array([False,False,False,False,True,True,False,False,False,False,False,False,False,False]),\n",
    "            'Value':np.array([False,False,False,False,False,False,True,True,False,False,False,False,False]),\n",
    "            '':np.array([True]*6 + [False]*6),\n",
    "        }\n",
    "# merged_pair_idxs = {}\n",
    "ex = exceptions()\n",
    "ex_rows = '|'.join(except_rows())\n",
    "df,merged_pair_idxs = _clean(file_path,except_rows=ex_rows,merged_pair_idxs=merged_pair_idxs)\n",
    "display(df)\n",
    "display(merged_pair_idxs)\n",
    "index_list = df.apply(\n",
    "    lambda row:row.astype(str).str.contains(stopping_criterion(None), case=False, na=False).any(),\n",
    "    axis=1\n",
    ")\n",
    "index_list_sum = index_list.sum()\n",
    "index_list_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex.get('2009-12-31/Schedule_of_Investments_12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Total Investments  4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$</td>\n",
       "      <td>536008160.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>$</td>\n",
       "      <td>486628831.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                     0   1   2   3   4   5   6   7    8  \\\n",
       "0           0                   NaN NaN NaN NaN NaN NaN NaN NaN  NaN   \n",
       "1           1  Total Investments  4 NaN NaN NaN NaN NaN NaN NaN    $   \n",
       "\n",
       "             9  10  11   12           13  14  \n",
       "0          NaN NaN NaN  NaN          NaN NaN  \n",
       "1  536008160.0 NaN NaN    $  486628831.0 NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(file_path)\n",
    "# df.apply(lambda col: col.astype(str).str.contains(r'total_*investments', case=False, regex=True)).any().any()\n",
    "display(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sec_windows",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
